[{"title":"Backstage.io","type":0,"sectionRef":"#","url":"/blog/tags/tech","content":"Thoughts from Q2 2023. Backstage.io I've been investigating backstage.io, both self-hosted on AWS and via a SaaS solution like roadie.io. Long story short, the deployment and configuration of backstage is far from trivial. There is an initial amount of knowledge required in order to set it up even vaguely for production. The documentation is ok to get you started, but has a long way to go. Getting running locally is painless, but the next step to deploy and configure as production is much more involved and the documentation is not clear. Depending on your IT policy, access to the backstage community on Discord may or may not be accessible. Things you have to do yourself with Backstage include configuring an authentication provider, which doesn't stop users logging in, that's another thing you have to do. Creating roles and a permissions policy is another thing you have to do from scratch. Spotify have a plugin bundle in beta which will be a paid-for offering, one of the things they have is RBAC and GUI configuration. I'd rather it was IaC, but there is also value in providing a simple interface for user management too. Productivity is an outcome, not a goal Research like this may be using the term productivity to get the attention of the c-suite, but the human factor matters more. Developer experience, and enablement is the goal for me. The happiness advantage is real. I'm reminded of the Inner game of tennis again, why do we work? Is it mastery, as suggested in Drive by Daniel Pink?","keywords":""},{"title":"Backstage.io","type":0,"sectionRef":"#","url":"/blog","content":"Thoughts from Q2 2023. Backstage.io I've been investigating backstage.io, both self-hosted on AWS and via a SaaS solution like roadie.io. Long story short, the deployment and configuration of backstage is far from trivial. There is an initial amount of knowledge required in order to set it up even vaguely for production. The documentation is ok to get you started, but has a long way to go. Getting running locally is painless, but the next step to deploy and configure as production is much more involved and the documentation is not clear. Depending on your IT policy, access to the backstage community on Discord may or may not be accessible. Things you have to do yourself with Backstage include configuring an authentication provider, which doesn't stop users logging in, that's another thing you have to do. Creating roles and a permissions policy is another thing you have to do from scratch. Spotify have a plugin bundle in beta which will be a paid-for offering, one of the things they have is RBAC and GUI configuration. I'd rather it was IaC, but there is also value in providing a simple interface for user management too. Productivity is an outcome, not a goal Research like this may be using the term productivity to get the attention of the c-suite, but the human factor matters more. Developer experience, and enablement is the goal for me. The happiness advantage is real. I'm reminded of the Inner game of tennis again, why do we work? Is it mastery, as suggested in Drive by Daniel Pink?","keywords":""},{"title":"Q2 2023 Tech diary","type":0,"sectionRef":"#","url":"/blog/2023-q2-tech-diary","content":"Thoughts from Q2 2023. Backstage.io I've been investigating backstage.io, both self-hosted on AWS and via a SaaS solution like roadie.io. Long story short, the deployment and configuration of backstage is far from trivial. There is an initial amount of knowledge required in order to set it up even vaguely for production. The documentation is ok to get you started, but has a long way to go. Getting running locally is painless, but the next step to deploy and configure as production is much more involved and the documentation is not clear. Depending on your IT policy, access to the backstage community on Discord may or may not be accessible. Things you have to do yourself with Backstage include configuring an authentication provider, which doesn't stop users logging in, that's another thing you have to do. Creating roles and a permissions policy is another thing you have to do from scratch. Spotify have a plugin bundle in beta which will be a paid-for offering, one of the things they have is RBAC and GUI configuration. I'd rather it was IaC, but there is also value in providing a simple interface for user management too. Productivity is an outcome, not a goal Research like this may be using the term productivity to get the attention of the c-suite, but the human factor matters more. Developer experience, and enablement is the goal for me. The happiness advantage is real. I'm reminded of the Inner game of tennis again, why do we work? Is it mastery, as suggested in Drive by Daniel Pink?","keywords":""},{"title":"Collections","type":0,"sectionRef":"#","url":"/docs/ansible/collections","content":"","keywords":""},{"title":"Installing Collections from a requirements.yml fileâ€‹","type":1,"pageTitle":"Collections","url":"/docs/ansible/collections#installing-collections-from-a-requirementsyml-file","content":"Example of how to specify roles and collection dependencies in a single requirements.yml file. --- roles: - src: https://github.com/dreamteam-gg/ansible-victoriametrics-role version: v0.0.1 name: victoria-metrics collections: - name: community.general version: &quot;1.2.0&quot;  "},{"title":"Documenting Collectionsâ€‹","type":1,"pageTitle":"Collections","url":"/docs/ansible/collections#documenting-collections","content":"ansible-docs displays documentation from docstrings in the python file which implements the Ansible Module or Plugin. https://docs.ansible.com/ansible/latest/dev_guide/developing_modules_documenting.html describes the expected format for the docstrings. While the documentation states &quot;...Any parsing errors will be obvious - you can view details by adding -vvv to the command.&quot;, my experience differs. The guidelines require adding # -*- coding: utf-8 -*- which I'm not a fan of as Python3 sets this by default, however if you are open-sourcing a Collection it must still be compatible with Python2. https://realpython.com/python-encodings-guide/#python-3-all-in-on-unicode "},{"title":"Testing module documentationâ€‹","type":1,"pageTitle":"Collections","url":"/docs/ansible/collections#testing-module-documentation","content":"See https://docs.ansible.com/ansible/latest/dev_guide/testing_documentation.html#testing-module-documentation "},{"title":"Modules and Pluginsâ€‹","type":1,"pageTitle":"Collections","url":"/docs/ansible/collections#modules-and-plugins","content":"A Module runs on the remote system, a Plugin runs on the Ansible Control node. See https://docs.ansible.com/ansible/latest/dev_guide/developing_locally.html#modules-and-plugins-what-s-the-difference "},{"title":"Troubleshootingâ€‹","type":1,"pageTitle":"Collections","url":"/docs/ansible/collections#troubleshooting","content":""},{"title":"'AnsibleSequence' object has no attribute 'get'â€‹","type":1,"pageTitle":"Collections","url":"/docs/ansible/collections#ansiblesequence-object-has-no-attribute-get","content":"I've hit this error a few times when developing a module or plugin for a collection. The errors in my case were related to incorrect indentation in the DOCUMENTATION string in the module or plugin source, which was not immediately apparent from the error message. "},{"title":"Extending Ansible","type":0,"sectionRef":"#","url":"/docs/ansible/extending-ansible","content":"","keywords":""},{"title":"Developer Guidesâ€‹","type":1,"pageTitle":"Extending Ansible","url":"/docs/ansible/extending-ansible#developer-guides","content":"There are lots of documents under the Developer Guide on the Ansible website. Here are some for my quick reference, do read them all though. https://docs.ansible.com/ansible/latest/dev_guide/developing_modules_general.htmlhttps://docs.ansible.com/ansible/latest/dev_guide/developing_plugins.htmlhttps://docs.ansible.com/ansible/latest/dev_guide/developing_inventory.htmlhttps://docs.ansible.com/ansible/latest/dev_guide/developing_program_flow_modules.htmlhttps://docs.ansible.com/ansible/latest/dev_guide/developing_module_utilities.htmlhttps://docs.ansible.com/ansible/latest/dev_guide/debugging.html "},{"title":"Best practiceâ€‹","type":1,"pageTitle":"Extending Ansible","url":"/docs/ansible/extending-ansible#best-practice","content":"https://docs.ansible.com/ansible/latest/dev_guide/developing_modules_best_practices.htmlhttps://docs.ansible.com/ansible/latest/dev_guide/developing_modules_documenting.html#documentation-block "},{"title":"Notesâ€‹","type":1,"pageTitle":"Extending Ansible","url":"/docs/ansible/extending-ansible#notes","content":"Don't trust the code in the Ansible repo as gospel. There are old modules which pre-date the current best-practice guidance.The DOCUMENTATION docstring is meta-information about the plugin you are writing. Changing the documented value of whether an option is required will actually enforce that when executing the plugin. The DOCUMENTATION docstring is also what determines the help text for the plugin when running ansible-doc.Not all examples will follow all the best practices. It will save you time and frustration in the long run to read the documentation first, and continually refer to it as you build a plugin. Adding no_log=True to a sensitive param on a custom module like a password does not present it from being logged if you enable DEBUG=True in ansible.cfg. See protecting-sensitive-data-with-no-log "},{"title":"Debugging modules in Collectionsâ€‹","type":1,"pageTitle":"Extending Ansible","url":"/docs/ansible/extending-ansible#debugging-modules-in-collections","content":"When writing new Ansible modules it can be difficult to determine where problems occur. Judicious use of display.debug helps, but there can still be times where you just don't know where or why something has gone wrong. Ansible will not display tracebacks even with DEBUG=True in ansible.cfg, so sometimes this is the only expedient way to discover where something unexpected has happened.  try: # module code here except Exception as e: display.debug(msg=traceback.format_exc()) raise e  "},{"title":"Inventory Plugins","type":0,"sectionRef":"#","url":"/docs/ansible/inventory-plugins","content":"Inventory Plugins All inventory plugins use a config file. Ini or Yaml inventory files are still configuration files, they just happen to contain all the hostnames and ip addresses for the ini and yaml plugins directly. Dynamic inventory plugins also need a config file, they all have a &quot;plugin:&quot; key which has to match the inventory filename. It is common for dynamic inventory plugins to have configuration in the contif file (which can be called anything) and for credentials to be read from environment variables. Use ansible-inventory -i &lt;config&gt;.yml --list --yaml to debug the inventory generated by a dynamic inventory script. Note that the --export flag can significantly change the inventory and even exclude variables which would otherwise be included in the inventory. For Collections, the [defaults] INVENTORY_PLUGINS key needs to be configured to specify the path to the folder where the inventory plugin .py file is located. I've not been able to get this working by just specifying the COLLECTIONS_PATH and inventory namespace.","keywords":""},{"title":"Filters","type":0,"sectionRef":"#","url":"/docs/ansible/filters","content":"","keywords":""},{"title":"Use-case - extract text from htmlâ€‹","type":1,"pageTitle":"Filters","url":"/docs/ansible/filters#use-case---extract-text-from-html","content":"This was developed to perform post-deployment verification against a RESTful service. The tomcat app startup script would always return before the application was ready to serve traffic and also there is a belt-and-braces check against a version endpoint to check that the url is serving the version on the application we believe should have just been deployed. The uri module handles getting what is unfortunately in this case html, then we need to extract the application version from the content. Cue Ansible Filters "},{"title":"regex_searchâ€‹","type":1,"pageTitle":"Filters","url":"/docs/ansible/filters#regex_search","content":"regex_search can perform the search and return the matched string. I won't share the whole html here, but note there are multiple versions in the html with no unique IDs on any of the elements. So how to extract just the application version? Although in python we can get this using a single regex and a match group, regex_search cannot return just the match group, only the whole match. &lt;li&gt;application version : 7.12.0-SNAPSHOT&lt;/li&gt;  We can do this in two simple steps using regex_search, the first filter matches the project version and sets that as a fact which we can search again and this time there is only one version number in the string to be searched and extracting the version number is now easy with a second regex.  - name: extract version element from response set_fact: deployed_application_version_element: &quot;{{ response | regex_search('application version : ([\\\\w\\\\.\\\\-]+)') }}&quot; - name: extract version string from element set_fact: deployed_application_version: &quot;{{ deployed_application_version_element | regex_search('(\\\\d+.*$)') }}&quot; - name: check facts debug: var: deployed_application_version  !!! note Ansible (2.6) does not support having both facts set in a single task. The following code prodeces the error below because the first fact is not set when the second fact tries to reference it. ```yaml - name: extract versions from response set_fact: deployed_application_version_element: &quot;{{ response | regex_search('application version : ([\\\\w\\\\.\\\\-]+)') }}&quot; deployed_application_version: &quot;{{ deployed_application_version_element | regex_search('(\\\\d+.*$)') }}&quot; ``` ```bash TASK [extract versions from response] ***************************************************************************************** fatal: [localhost]: FAILED! =&gt; {&quot;msg&quot;: &quot;Unexpected templating type error occurred on ({{ deployed_application_version_element | regex_search('(\\\\\\\\d+.*$)') }}): expected string or buffer&quot;} ``` The error is clearer if we drop the regex_search filter from the second fact, although as always we must read the Ansible error message carefully and fully. ```yaml - name: extract versions from response set_fact: deployed_application_version_element: &quot;{{ response | regex_search('application version : ([\\\\w\\\\.\\\\-]+)') }}&quot; deployed_application_version: &quot;{{ deployed_application_version_element }}&quot; ``` ```bash TASK [extract versions from response] ***************************************************************************************** fatal: [localhost]: FAILED! =&gt; {&quot;msg&quot;: &quot;The task includes an option with an undefined variable. The error was: 'deployed_project_version_match' is undefined\\n\\nThe error appears to have been in '/Users/agar/code/agarthetiger/ansible/check-version.yml': line 9, column 7, but may\\nbe elsewhere in the file depending on the exact syntax problem.\\n\\nThe offending line appears to be:\\n\\n\\n - name: extract versions from response\\n ^ here\\n&quot;}  "},{"title":"regex_replaceâ€‹","type":1,"pageTitle":"Filters","url":"/docs/ansible/filters#regex_replace","content":"It would be elegant to get this in one step and we can do that with regex_replace, where the replacement string can reference match groups. We need to modify the original regex slightly so the entire string is matched, in order for just the replacement match group to become the returned string. Lets break the regex down. The leading ^.* and trailing .*?$ ensure that the whole string is matched and replaced by the match groupapplication version : matches the text in the list html element for the version we're interested in ([\\w\\.\\-]) matches any word character (letters, numbers and underscore) plus dot and hyphen and the round brackets around this expression mark it as the first (and only) match group.  - name: extract application version from response set_fact: deployed_application_version: &quot;{{ response | regex_replace('^.*application version : ([\\\\w\\\\.\\\\-]+).*?$', '\\\\1') }}&quot;  Online tools like pythex.org can be useful for quickly testing regular expressions. !!! danger Be very careful about what you paste into online tools like this. Always triple check that you are never sharing anything sensitive and if in doubt test a rexeg locally, even if it is more cumbersome. Security is never worth risking for speed. "},{"title":"Use-case - search for text in jsonâ€‹","type":1,"pageTitle":"Filters","url":"/docs/ansible/filters#use-case---search-for-text-in-json","content":"A RESTful API returned a json payload which we needed to search through for a name stored in a variable. This didn't work, the variable {{ check_control_plane }} was never interpolated so the name was never found even when it was present. - name: set fact with controlplane check set_fact: pingdom_controlplane_check: &quot;{{ pingdom_checks.json.checks | json_query('[?name==`{{ check_controlplane_name }}`]') | list }}&quot;  This does work as expected.  - name: set fact with controlplane check set_fact: pingdom_controlplane_check: &quot;{{ pingdom_checks.json.checks | json_query(query) | list }}&quot; vars: query: &quot;[?name=='{{ check.controlplane.name }}']&quot;  "},{"title":"Lookup Plugins","type":0,"sectionRef":"#","url":"/docs/ansible/lookup-plugins","content":"","keywords":""},{"title":"Examplesâ€‹","type":1,"pageTitle":"Lookup Plugins","url":"/docs/ansible/lookup-plugins#examples","content":""},{"title":"with_fileglobâ€‹","type":1,"pageTitle":"Lookup Plugins","url":"/docs/ansible/lookup-plugins#with_fileglob","content":"Use with_fileglob with a file lookup to use the file contents.  - name: ensure CRDs are present k8s: state: present definition: &quot;{{ lookup('file', item) }}&quot; with_fileglob: - &quot;files/crd-*.yml&quot;  "},{"title":"SELinux","type":0,"sectionRef":"#","url":"/docs/ansible/selinux","content":"","keywords":""},{"title":"delegate_to: localhostâ€‹","type":1,"pageTitle":"SELinux","url":"/docs/ansible/selinux#delegate_to-localhost","content":"Tasks which use delegate_to: localhost will run on the Ansible control node. This node can have selinux enabled and enforcing. "},{"title":"Ansible in a virtual environmentâ€‹","type":1,"pageTitle":"SELinux","url":"/docs/ansible/selinux#ansible-in-a-virtual-environment","content":"If you have installed and are running Ansible from a virtual environment (venv) then when a playbook uses delegate_to: localhost it is the same venv python interpreter that will execute the module locally. "},{"title":"Python2â€‹","type":1,"pageTitle":"SELinux","url":"/docs/ansible/selinux#python2","content":"If you are using Ansible in a virtual environment (venv) then the venv python interpreter will not have the selinux python packages installed from libselinux-python. There is a python shim in pypi called selinux which should provide the selinux bindings for Ansible running in a python2 venv. "},{"title":"Python3â€‹","type":1,"pageTitle":"SELinux","url":"/docs/ansible/selinux#python3","content":"As far as I've found there is no python3 equivalent. Workarounds from the internet include explicitly setting the python_interpreter for localhost in your inventory to the system version of python, or copying in the selinux folders from the system python's site-packages folder. "},{"title":"Setting the localhost python interpreterâ€‹","type":1,"pageTitle":"SELinux","url":"/docs/ansible/selinux#setting-the-localhost-python-interpreter","content":"The localhost interpreter option should work if you are running Ansible with python2 on CentOS/RHEL7 and are not using a dynamic inventory source. You may have to use ansible-inventory -i &lt;inventory_script_or_plugin&gt; --list --yaml --output inventory/temp_statis_inventory.yml to create a temporary file on disk with the inventory, in order to inject the localhost setting 3. "},{"title":"Python, CentOS and RHELâ€‹","type":1,"pageTitle":"SELinux","url":"/docs/ansible/selinux#python-centos-and-rhel","content":"Both CentOS 7 and RHEL 7 ship with python2 by default and both have a libselinux-python package available which installs selinux python packages in the system version of python, which is python2. Likewise CentOS 8 and RHEL 8 have a libselinux-python3 package which installs the python selinux support for python3. Although there are python36 packages available for CentOS 7 and RHEL 7 there are no plans to provide libselinux-python3 for either CentOS 7 1 or RHEL 7 2. "},{"title":"Playbooks","type":0,"sectionRef":"#","url":"/docs/ansible/playbooks","content":"","keywords":""},{"title":"Conditional checksâ€‹","type":1,"pageTitle":"Playbooks","url":"/docs/ansible/playbooks#conditional-checks","content":"Conditional checks use the when: syntax. When conditions can use raw Jinja2 expressions but can execute regular python code so can access methods like String.find() to check for a text match in a String. Multiple conditions should be enclosed with parenthesis, multiple conditions can be specified in a list where they are all required to be true (logical AND). "},{"title":"Conditional check examplesâ€‹","type":1,"pageTitle":"Playbooks","url":"/docs/ansible/playbooks#conditional-check-examples","content":"when: - tomcat_status_result.stdout.find(&quot;JVM is running.&quot;) &gt; -1 - tomcat_status_result.stderr != &quot;&quot; - tomcat_status_result.rc == 0 --- - hosts: all tasks: - name: &quot;print inventory vars&quot; debug: var: &quot;{{ item }}&quot; with_items: - inventory_dir - inventory_file when: inventory_dir | regex_search('dev$') - hosts: all tasks: - name: &quot;apply stub role&quot; include_role: name: issuer-wallet-stub when: inventory_dir | regex_search('dev$')  "},{"title":"Required variablesâ€‹","type":1,"pageTitle":"Playbooks","url":"/docs/ansible/playbooks#required-variables","content":"Often at the beginning of a Playbook or Role you may want to assert that all mandatory variables have been defined. This can avoid more difficult problems further into a playbook, like only finding this out mid-way through a production deployment. vars.yml REQUIRED_VARS: - application_version # Exists - i_dont_exist  - name: Ensure required variables have been set assert: that: lookup('vars', item) is defined loop: &quot;{{ REQUIRED_VARS }}&quot; delegate_to: localhost run_once: Yes  By using a list of varaible names in vars.yml for example in an Ansible Role, it keeps all the variable declarations together. Note that you cannot use is defined directly on {{ item }}, you will get the following output (v2.10.4). ok: [localhost] =&gt; (item=i_dont_exist) =&gt; { &quot;ansible_loop_var&quot;: &quot;item&quot;, &quot;i_dont_exist&quot;: &quot;VARIABLE IS NOT DEFINED!&quot;, &quot;item&quot;: &quot;i_dont_exist&quot; }  "},{"title":"Filtersâ€‹","type":1,"pageTitle":"Playbooks","url":"/docs/ansible/playbooks#filters","content":"See documentation on filters. Filters use Jinja2, and Ansible ships with some extra ones to those available in Jinja2. Remember that if using a filter in a conditional statement that python methods are also accessible. Also note that the online Jinja2 documentation doesn't go back to python-jinja2 2.7 which is what is provided in RedHat repos for RHEL7. There are no RHEL plans to update Jinja2 to anything later, so what you read on the Jinja website will include features not available to Ansible on RHEL7, because of the Jinja2 version. To select an item from a list, based on an attribute, use the selectattr with the match filter, as the equalsto filter is only available in Jinja2 2.8. Given a yaml file like this,  projects: - name: project-a version: '2.3.4-SNAPSHOT' - name: project-b version: '1.2.3-SNAPSHOT' - name: project-c value: '5.6.7-SNAPSHOT'  Select the version of project-b using the following expression.  - hosts: project-hosts-b vars: - deployable_version: &quot;{{ projects | selectattr('name', 'match', '^project-b$') | map(attribute='version') | list | first }}&quot;  References jinja2-selectattr-filter "},{"title":"Debugâ€‹","type":1,"pageTitle":"Playbooks","url":"/docs/ansible/playbooks#debug","content":"You can print a message with variable information in it, combine this with the verbosity level, below which the debug will not output anything. - debug: msg: &quot;System {{ inventory_hostname }} has gateway {{ ansible_default_ipv4.gateway }}&quot; verbosity: 4 when: ansible_default_ipv4.gateway is defined  See documentation for the debug module. "},{"title":"Play optionsâ€‹","type":1,"pageTitle":"Playbooks","url":"/docs/ansible/playbooks#play-options","content":"Disable facts gathering - hosts: all gather_facts: false  "},{"title":"Import and Includeâ€‹","type":1,"pageTitle":"Playbooks","url":"/docs/ansible/playbooks#import-and-include","content":"Ansible documentation on Creating reusable playbooksNote there are trade-offs when selecting between static and dynamic, any import* tasks will be static, any include* tasks will be dynamic. All import* statements are pre-processed at the time playbooks are parsed.All include* statements are processed as they encountered during the execution of the playbook. "},{"title":"Roles","type":0,"sectionRef":"#","url":"/docs/ansible/roles","content":"","keywords":""},{"title":"Embedding modules and pluginsâ€‹","type":1,"pageTitle":"Roles","url":"/docs/ansible/roles#embedding-modules-and-plugins","content":"Ansible Roles can also include embedded modules and plugins, so you don't necessarily need an Ansible Collection if you need to use a custom module or plugin in a Role. See https://docs.ansible.com/ansible/latest/user_guide/playbooks_reuse_roles.html#embedding-modules-and-plugins-in-roles "},{"title":"Ansible tools","type":0,"sectionRef":"#","url":"/docs/ansible/tools","content":"","keywords":""},{"title":"Ansible Run Analysis (ARA)â€‹","type":1,"pageTitle":"Ansible tools","url":"/docs/ansible/tools#ansible-run-analysis-ara","content":"See the ARA article or view the code on GitHub. "},{"title":"Supportâ€‹","type":1,"pageTitle":"Ansible tools","url":"/docs/ansible/tools#support","content":"Ansible Google GroupAnsible Source and Releases on GitHub "},{"title":"Local actions","type":0,"sectionRef":"#","url":"/docs/ansible/local-actions","content":"","keywords":""},{"title":"Single moduleâ€‹","type":1,"pageTitle":"Local actions","url":"/docs/ansible/local-actions#single-module","content":"ansible all -m setup -i 192.168.0.11,  ansible all execute ansible against host group 'all' -m setup run the setup module to gather information about the hosts-i 192.168.0.11, inline host list, must be comma-separated list of hosts so add training comma for a single host. See Examples on the Ansible Setup module documentation. "},{"title":"Local actionsâ€‹","type":1,"pageTitle":"Local actions","url":"/docs/ansible/local-actions#local-actions-1","content":"If you want to run a playbook and always do things locally, execute the playbook using --connection=local on the command line or connection: local on the play. This instructs ansible not to establish an SSH connection. This is useful if local loobpack is disabled eg. due to firewall rules or security groups. To run a playbook which uses - hosts: all or similar, pass an inventory with just localhost or 127.0.0.1 and connection=local ansible-playbook -i 127.0.0.1, --connection=local myplaybook.yaml  Alternatively, within a playbook with tasks to be executed remotely, include a section with - hosts: 127.0.0.1 connection: local  More alternatives include using the local_action module which is a shorthand syntax for the delegate_to: 127.0.0.1 option. Combine these with run_once: true to ensure things running locally only happen once if that's what you want (like downloading a file once to then use Ansible to push the file to multiple hosts). See Ansible documentation on Delegation This is useful to switch to running selected tasks locally in the middle of a role execution for example, where you cannot add hosts: directives to switch between local and remote execution. - hosts: all gather_facts: false tasks: - name: ensure required parameters have been set fail: msg=&quot;Variable '{{ item }}' is not defined&quot; when: item not in vars with_items: - public_key_file - host_user_id run_once: true - name: locate public key file local_action: module: stat path: &quot;{{ public_key_file }}&quot; register: keyFile run_once: true  "},{"title":"Localhost and non-SSH connectionsâ€‹","type":1,"pageTitle":"Local actions","url":"/docs/ansible/local-actions#localhost-and-non-ssh-connections","content":"ansible_connection=local can be used in the inventory or on the command line to specify how Ansible will connect to the target host. In the case of local actions you can use ansible_connection=local as an inventory parameter with localhost to execute something locally even if you don't have a local loopback connection. The equivalent in a playbook is connection: local. Note localhost will be recognised by Ansible as a valid host if the /etc/hosts file has an entry for 127.0.0.1 localhost or if localhost 127.0.0.1 ansible_connection=local is specified in the inventory, otherwise use 127.0.0.1. "},{"title":"Ansible Tower","type":0,"sectionRef":"#","url":"/docs/ansible/tower","content":"","keywords":""},{"title":"AWXâ€‹","type":1,"pageTitle":"Ansible Tower","url":"/docs/ansible/tower#awx","content":"AWX is the upstream open-source version of Ansible Tower. See the AWX Project for details. You can get it from GitHub. "},{"title":"API Gateway","type":0,"sectionRef":"#","url":"/docs/aws/api-gateway","content":"","keywords":""},{"title":"Lambda Authorizersâ€‹","type":1,"pageTitle":"API Gateway","url":"/docs/aws/api-gateway#lambda-authorizers","content":"Lambda Authorizers can be configured with Identity Sources, which are effectively mandatory inputs. They can be query string params, HTTP request headers, etc. API Gateway will reject requests with a 401 response if a request does not contain the mandatory Identity Sources. To use a lambda authorizer which uses one or another method of authentication, specify a blank list for the Identity Sources instead. See https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html "},{"title":"CloudFront Functions","type":0,"sectionRef":"#","url":"/docs/aws/cloudfront-functions","content":"","keywords":""},{"title":"Javascript Runtimeâ€‹","type":1,"pageTitle":"CloudFront Functions","url":"/docs/aws/cloudfront-functions#javascript-runtime","content":"The CloudFront Functions Javascript runtime 2.0 is compliant with ECMAScript v5.1 and supports some features of v6 through to v12. There are some limitations which there seem to be no documentation for. You can't loop over more than 100 things (limit hit with loop over redirects). See the AWS docs for the Javascript Runtime. "},{"title":"Monitoringâ€‹","type":1,"pageTitle":"CloudFront Functions","url":"/docs/aws/cloudfront-functions#monitoring","content":""},{"title":"Metricsâ€‹","type":1,"pageTitle":"CloudFront Functions","url":"/docs/aws/cloudfront-functions#metrics","content":"Under CloudFront -&gt; Telemetry -&gt; Monitoring you can see metrics like throttles, errors and the cpu usage (scaled from 0 to 100). At or above 100% cpu usage, which is really execution time, your code will get terminated and the request will proceed to the CloudFront cache regardless. This means CF Functions should not be relied upon for any critical access control or restrictions on content. "},{"title":"Logsâ€‹","type":1,"pageTitle":"CloudFront Functions","url":"/docs/aws/cloudfront-functions#logs","content":"CF Function logs are always sent to us-east-1. The log group name is in the format /aws/cloudfront/function/FunctionName, where FunctionName is the name that you gave to the function when you created it. See https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/edge-functions-logs.html#cloudfront-function-logs "},{"title":"Quotasâ€‹","type":1,"pageTitle":"CloudFront Functions","url":"/docs/aws/cloudfront-functions#quotas","content":"Cloudfront Functions and KV Stores have more restrictive limits than Lambda@Edge, many of which cannot be increased. CloudFront Functions Quotas "},{"title":"Troubleshootingâ€‹","type":1,"pageTitle":"CloudFront Functions","url":"/docs/aws/cloudfront-functions#troubleshooting","content":"Apparently if you misconfigure a CloudFront Distribution to have multiple Subject Alternate Names (SAN) but missing one specific domain, an ACM certificate valid for all SANs, all Route53 records correct, and a CloudFront Function attached to the default Behaviour as a Viewer Request, the Function will get called correctly for all domains except the one missing from the Distribution. Ask me how I know ðŸ˜‚ "},{"title":"CloudFront","type":0,"sectionRef":"#","url":"/docs/aws/cloudfront","content":"","keywords":""},{"title":"CloudFront Distributions and alternate namesâ€‹","type":1,"pageTitle":"CloudFront","url":"/docs/aws/cloudfront#cloudfront-distributions-and-alternate-names","content":"When you try to add an alternate domain name to a distribution but the alternate domain name is already in use on a different distribution, you get a CNAMEAlreadyExists error (One or more of the CNAMEs you provided are already associated with a different resource). This means you cannot simply build a new Cloudfront Distribution for a site which is already live if you want to perform a migration for any reason. A migration is probably still possible, however it requires several manual steps in order to perform the migration to both the source and target CloudFront Distributions and DNS. See https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/alternate-domain-names-move.html for more information. "},{"title":"Elastic Block Storage (EBS)","type":0,"sectionRef":"#","url":"/docs/aws/ebs","content":"","keywords":""},{"title":"Referencesâ€‹","type":1,"pageTitle":"Elastic Block Storage (EBS)","url":"/docs/aws/ebs#references","content":"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-storage-gp3-migration-selection.htmlhttps://docs.aws.amazon.com/ebs/latest/userguide/ebs-io-characteristics.html#ebs-io-metrics "},{"title":"Vars and Facts","type":0,"sectionRef":"#","url":"/docs/ansible/vars-and-facts","content":"","keywords":""},{"title":"Varsâ€‹","type":1,"pageTitle":"Vars and Facts","url":"/docs/ansible/vars-and-facts#vars","content":""},{"title":"Magic variablesâ€‹","type":1,"pageTitle":"Vars and Facts","url":"/docs/ansible/vars-and-facts#magic-variables","content":"Commonly used magic variables include hostvars, groups, group_names, and inventory_hostname. groups is a list of all the groups (and hosts) in the inventory. inventory_dir is the pathname of the directory holding Ansibleâ€™s inventory host fileinventory_file is the pathname and the filename pointing to the Ansibleâ€™s inventory host file.inventory_hostname is the name of the target host for the current play. This is the remote target even when the task uses delegate_to.inventory_hostname_short is useful if your host has a long fqdn and you only need the hostname up to the first period. Note this comes from the inventory, not from the target host, useful if gather_facts is disabled.ansible_hostname is a discovered fact from the remote host, so not available with gather_facts disabled. See the ansible docs for more details. "},{"title":"Group and Host Variablesâ€‹","type":1,"pageTitle":"Vars and Facts","url":"/docs/ansible/vars-and-facts#group-and-host-variables","content":"See documentation for playbooks best practices. inventory/group_vars/all.yaml my_var_name: &quot;some value&quot; Use {{ hostvars[inventory_hostname]['my_var_name'] }} to reference. Note the group_vars/all structure does not mean there is an [all] hosts group to reference, but the vars in all will get applied to all group (hosts), and can be referenced using the magic variable inventory_hostname. Note the use of single quotes for the my_var_name (which is looking up the string 'my_var_name') vs the [inventory_hostname] syntax to index the inventory_hostname in the hostvars array. "},{"title":"Load variables from a fileâ€‹","type":1,"pageTitle":"Vars and Facts","url":"/docs/ansible/vars-and-facts#load-variables-from-a-file","content":"On the command line, use -e or --extra-vars. Set additional variables as key=value or YAML/JSON, if filename prepend the filename with @. ansible-playbook ... --extra-vars '@my-vars.yaml'  Combine vars from a file and additional vars by using --extra-vars twice. ansible-playbook ... --extra-vars '@my-vars.yaml' --extra-vars 'target_env=uat generate_report=true'  See documentation "},{"title":"Factsâ€‹","type":1,"pageTitle":"Vars and Facts","url":"/docs/ansible/vars-and-facts#facts","content":""},{"title":"Setup moduleâ€‹","type":1,"pageTitle":"Vars and Facts","url":"/docs/ansible/vars-and-facts#setup-module","content":"Basic example of running the setup module locally. ansible -m setup all -i localhost, --connection=local "},{"title":"Sample Factsâ€‹","type":1,"pageTitle":"Vars and Facts","url":"/docs/ansible/vars-and-facts#sample-facts","content":"Quick reference for some of the facts I commonly use. Fact\tValues (not exhaustive list)ansible_os_family\t'RedHat', 'Darwin', Debian' ansible_distribution\t'CentOS', 'MacOSX', 'Ubuntu' ansible_distribution_major_version\t'7' ansible_distribution_version\t'7.4.1804' ansible_pkg_mgr\t'yum', 'apt', 'homebrew' ansible_system\t'Linux', 'Darwin' ansible_lsb\tdict containing id (ansible_distribution), release (ansible_distribution_version) and others. Depends on lsb package being installed. ansible_hostname ansible_fqdn ansible_env ansible_dns.nameservers\tList of name server IPs ansible_dns.search\tList of domains to search ansible_domain\t Access facts per host by just referencing the fact name as a variable. They are also available via {{ ansible_facts['fact_name'] }} or hostvars[inventory_hostname]['fact_name']. Note that ansible_facts is a subset of hostvars[inventory_hostname]. --- - hosts: localhost connection: local tasks: - name: print ansible_facts debug: var: ansible_facts - name: print all host facts debug: var: hostvars[inventory_hostname]  "},{"title":"Elastic Cloud Compute (EC2)","type":0,"sectionRef":"#","url":"/docs/aws/ec2","content":"","keywords":""},{"title":"Instance Typesâ€‹","type":1,"pageTitle":"Elastic Cloud Compute (EC2)","url":"/docs/aws/ec2#instance-types","content":"T type instances are general purpose with a baseline CPU performance and burstable to higher limits. The Unlimited option allows bursting to above baseline performance for an unlimited amount of time. CPU credits are still accumulated for time spend below the baseline performance. You can choose between Unlimited to ensure the CPU will never be throttled or Standard to ensure you're billing is predictable, at the expense of being throttled when CPU credits expire. "},{"title":"Referencesâ€‹","type":1,"pageTitle":"Elastic Cloud Compute (EC2)","url":"/docs/aws/ec2#references","content":"https://aws.amazon.com/ec2/instance-types/ "},{"title":"Bash reference","type":0,"sectionRef":"#","url":"/docs/bash/reference","content":"","keywords":""},{"title":"Process substitutionâ€‹","type":1,"pageTitle":"Bash reference","url":"/docs/bash/reference#process-substitution","content":"This example was used to upload a file to post a file to a site requiriing authentication, without ever writing the credentials to disk or having them exposed in the command history or the running process list while the command is executing. Note that this code was run from a Jenkinsfile pipeline shared library, so the ${} variables are coming from Jenkins (Groovy) DSL. response_code=\\$(curl -w %{http_code} --netrc-file &lt;(cat &lt;&lt;&lt;'machine $uploadDomain login ${Username} password ${Password}') -sS --upload-file ${filename} '${uploadUrl}')  Breaking this command down &lt;&lt;&lt;'Something here' is a here string, a variant of a here doc where the (variables in the) string are expanded before being fed to standard input. &lt;( command ) is process substitution where the standard output of one process can be fed to the standard input of another process. In this case we're using process substitution and cat to feed curl with a netrc 'file' without ever writing the file to disk. --netrc-file is a curl option which can be used to provide credentials for curl to present when connecting to specified domains. "},{"title":"Commentsâ€‹","type":1,"pageTitle":"Bash reference","url":"/docs/bash/reference#comments","content":"The # symbol is a bash comment, and can also be used to to stop processing of command line options. This is useful in git alias commands where we want to reference command line args in the git alias. Without the hash at the end of the alias below, using GitBash on Windows 10, the code will error with a message like grep: develop: No such file or directory. Adding the comment halts processing and the command works as expected. alias.prune-merged=!git branch --merged &quot;$1&quot; |grep -v -e develop -e master -e &quot;$1&quot; #  "},{"title":"Explain Shellâ€‹","type":1,"pageTitle":"Bash reference","url":"/docs/bash/reference#explain-shell","content":"Explain Shell can be useful to start to understand complex bash commands. !!! warning Remember this is a public (untrusted) website so be very careful about what you paste in here. "},{"title":"Markdown","type":0,"sectionRef":"#","url":"/docs/cheat-sheets/markdown","content":"","keywords":""},{"title":"Reference Linksâ€‹","type":1,"pageTitle":"Markdown","url":"/docs/cheat-sheets/markdown#reference-links","content":"The implicit link name shorcut allows the 2nd set of square brackets to be empty and the id should then be the same to the link name, ie the text in the first square brackets, although the ids are not case sensitive. [My Link text][ref_id] ... [ref_id]: https://mylink.com &quot;Optional link title&quot;  "},{"title":"Referencesâ€‹","type":1,"pageTitle":"Markdown","url":"/docs/cheat-sheets/markdown#references","content":""},{"title":"Jenkins","type":0,"sectionRef":"#","url":"/docs/cheat-sheets/jenkins","content":"","keywords":""},{"title":"Jenkinsfile Pipelinesâ€‹","type":1,"pageTitle":"Jenkins","url":"/docs/cheat-sheets/jenkins#jenkinsfile-pipelines","content":"http://localhost:8080/pipeline-syntax/ - The Pipeline Syntax Snippet Generator, almost always generates valid Jenkins Pipeline DSL :)http://localhost:8080/pipeline-syntax/globals - Global 'variables' docs. In a shared or global library, add a .txt file alongside a .groovy file with text or HTML to automatically have add help to this globals page.http://localhost:8080/pipeline-syntax/gdsl - GDSL, auto-complete Jenkins DSL in IntelliJ "},{"title":"JobDSLâ€‹","type":1,"pageTitle":"Jenkins","url":"/docs/cheat-sheets/jenkins#jobdsl","content":"http://localhost:8080/plugin/job-dsl/api-viewer/index.html# - JobDSL API documentation, based on the installed plugins on the Jenkins master. "},{"title":"JCasCâ€‹","type":1,"pageTitle":"Jenkins","url":"/docs/cheat-sheets/jenkins#jcasc","content":"http://localhost:8080/configuration-as-code/reference - Configration as code reference based on the installed (CasC) plugins and versions. Note not all of the available options are listed here, like the root jobs: option.http://localhost:8080/configuration-as-code/schema - JSON schema for the CasC options. Less descriptive and less readable but more complete than the above reference. "},{"title":"External Referencesâ€‹","type":1,"pageTitle":"Jenkins","url":"/docs/cheat-sheets/jenkins#external-references","content":"https://javadoc.jenkins-ci.org/ - Java doc for Jenkins API "},{"title":"Python Quick Reference","type":0,"sectionRef":"#","url":"/docs/cheat-sheets/python","content":"","keywords":""},{"title":"Regular Expressionsâ€‹","type":1,"pageTitle":"Python Quick Reference","url":"/docs/cheat-sheets/python#regular-expressions","content":"+ - match the previous RE one or more times? - match the previous RE zero or one time only eg ab? matches a or ab*?, +? - non-greedy match. eg For the string &lt;a&gt;something&lt;b&gt;, &lt;.*&gt; will match &lt;a&gt;something&lt;b&gt;, using &lt;.*?&gt; this will match &lt;a&gt;.\\. - Escape matching a dot, eg r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}' for a basic ip address expression "},{"title":"Referencesâ€‹","type":1,"pageTitle":"Python Quick Reference","url":"/docs/cheat-sheets/python#references","content":"Python 3 re documentation "},{"title":"Ansible Cheat Sheet","type":0,"sectionRef":"#","url":"/docs/cheat-sheets/ansible","content":"","keywords":""},{"title":"Debuggingâ€‹","type":1,"pageTitle":"Ansible Cheat Sheet","url":"/docs/cheat-sheets/ansible#debugging","content":"{{ inventory_hostname }} The inventory name for the current host being iterated over in the task. One of Ansible's Special Variables. {{ ansible_facts }} Dictionary of facts gathered or cached for the inventory_hostname. "},{"title":"Print hostvarsâ€‹","type":1,"pageTitle":"Ansible Cheat Sheet","url":"/docs/cheat-sheets/ansible#print-hostvars","content":"- name: print hostvars debug: var: hostvars[inventory_hostname] # This is the same as {{ ansible_facts }} but note that ansible_facts will work differntly to hostvars[inventory_hostname] when delegating actions to another host. verbosity: 2  Note that hostvars[inventory_hostname] will always give the facts for the specified inventory_hostname, whereas ansible_facts gives it for the machine the task is being run on. This behaviour will be apparent when delegating tasks to other machines, especially localhost which will not have had any facts gathered at all. "},{"title":"Flow controlâ€‹","type":1,"pageTitle":"Ansible Cheat Sheet","url":"/docs/cheat-sheets/ansible#flow-control","content":"Run a single task in serial in a Role. Note that the serial keyword is not applicable to a task, so we have to use a workaround with a side-effect which is that the host selected by Ansible to execute the run_once task will report as being changed even when the actual change took place on the host which the task was delegated to. - name: Enable and start the service service: name: my_service enabled: yes state: started delegate_to: &quot;{{ item }}&quot; run_once: true loop: &quot;{{ ansible_play_batch }}&quot;  "},{"title":"Loopsâ€‹","type":1,"pageTitle":"Ansible Cheat Sheet","url":"/docs/cheat-sheets/ansible#loops","content":"Loops work with when and failed_when as well as the until conditions. when will be evaluated once prior to entering the loop, until will be evaluated as the loop repetition condition and finally the failed_when condition will be evaluated a single time only once the until condition is true or the loop has reached the maximum retry limit.  - name: Poll for Cassandra connection shell: &quot;echo 'SHOW HOST;' | cqlsh {{ ansible_default_ipv4.address }} -u 'cassandra' --ssl&quot; register: result until: result.stderr.find(&quot;Connection refused&quot;) == -1 retries: 10 delay: 10 changed_when: false failed_when: result.stderr.find(&quot;Connection refused&quot;) != -1  "},{"title":"Blocksâ€‹","type":1,"pageTitle":"Ansible Cheat Sheet","url":"/docs/cheat-sheets/ansible#blocks","content":"Block sections include block, rescue and always. These function like try, catch and finally respectively. Blocks can accept options like run_once and tags, note that they can't be used to apply handlers which must be specified per task. "},{"title":"Process files on hostâ€‹","type":1,"pageTitle":"Ansible Cheat Sheet","url":"/docs/cheat-sheets/ansible#process-files-on-host","content":"The find module can be used to find files on a remote host and then process them in later in the playbook by registering the output. The full filename and path is stored in the registered variable under files.path, which can be looped over as per the example below. - name: find all logstash plugins find: paths: &quot;{{ logstash_plugins_folder }}&quot; patterns: &quot;*.gem&quot; recurse: no register: plugin_gems changed_when: no - name: ensure all plugins are installed logstash_plugin: name: &quot;{{ item.path }}&quot; state: present loop: &quot;{{ plugin_gems.files }}&quot; notify: restart logstash  "},{"title":"Documentationâ€‹","type":1,"pageTitle":"Ansible Cheat Sheet","url":"/docs/cheat-sheets/ansible#documentation","content":"Special VariablesKeywordsFiltersJinja2 Filters "},{"title":"Vagrant","type":0,"sectionRef":"#","url":"/docs/cheat-sheets/vagrant","content":"","keywords":""},{"title":"Tipsâ€‹","type":1,"pageTitle":"Vagrant","url":"/docs/cheat-sheets/vagrant#tips","content":""},{"title":"Ansible with Vagrantâ€‹","type":1,"pageTitle":"Vagrant","url":"/docs/cheat-sheets/vagrant#ansible-with-vagrant","content":"To reuse Vagrant VMs as the target for Ansible automation development, you can use a git submodule to include the repo with the Vagrantfile. In the Vagrantfile include an ansible provisioner then the provisioner will create an Ansible Inventory in .vagrant/provisioners/ansible/inventory which can then be used as the Ansible inventory from playbooks under development on the host, without having to copy/duplicate any files. Note that only the ansible provisioner creates this inventory, presumably because the provisioner itself is using this to provision the VMs from the host machine with Ansible. The ansible-local provisioner runs on the VMs so does not create an inventory on the Vagrant host. Multiple provisioners can be specified in the Vagrantfile, the inventory trick with the Ansible provisioner can co-exist alongside configuration with the ansible-local provisioner to further provision the VMs on startup should this be required. Ideally the base box would be built with Packer so that no time-consuming customisation is required when starting the VMs. Yes, Docker would be much faster, however there are times when a VM more closely replicates the target environment for the Ansible automation and this is desirable. "},{"title":"Sienci Mill One","type":0,"sectionRef":"#","url":"/docs/cnc/sienci-mill-one","content":"","keywords":""},{"title":"Resourcesâ€‹","type":1,"pageTitle":"Sienci Mill One","url":"/docs/cnc/sienci-mill-one#resources","content":"Sienci recommended feeds and speedsSienci Mill One BOM spreadsheet "},{"title":"Docusaurus - First impressions","type":0,"sectionRef":"#","url":"/docs/docusaurus/first-impressions","content":"","keywords":""},{"title":"Blogsâ€‹","type":1,"pageTitle":"Docusaurus - First impressions","url":"/docs/docusaurus/first-impressions#blogs","content":"I was hoping to get away from needing to have the date-time in the filename. "},{"title":"Stylingâ€‹","type":1,"pageTitle":"Docusaurus - First impressions","url":"/docs/docusaurus/first-impressions#styling","content":"Overall for absolutely no effort on my part the docusaurus-generated site looks really nice. "},{"title":"Navigationâ€‹","type":1,"pageTitle":"Docusaurus - First impressions","url":"/docs/docusaurus/first-impressions#navigation","content":"Where is the navigation for the &quot;site&quot;? Seems odd that by default there is no nav shown on the homepage. Maybe I need to create some folders first... "},{"title":"GitHub Pagesâ€‹","type":1,"pageTitle":"Docusaurus - First impressions","url":"/docs/docusaurus/first-impressions#github-pages","content":"Errors on the page suggest I've not got the baseUrl configured correctly. I definitely hadn't to start with, but it should be right now but still seeing css fail to load. Fixed now with a whitespace poke at the index.html file, probably me changing the GitHub Pages configuration for this repository caused the latest code on the gh-pages branch to not be deployed to wherever GitHub serves static sites from. I'll chalk that one up to GitHub and not Docusaurus. "},{"title":"Build the right thing...","type":0,"sectionRef":"#","url":"/docs/intro","content":"Build the right thing... ... and build the thing right This site contains a tiny sample of my notes and experience I've gathered over my professional career as a software developer and DevOps engineer. Find me on LinkedIn. It started as a weekend project to learn about MKDocs and TravisCI. I've used this intermittently, I tend to favour my own python cli tool hint these days, but have copied the contents over into this new Docusaurus site as another weekend project. Site built with Docusaurus and GitHub Actions About Me Andrew Garner Open Source A lightweight Jenkins Shared Library test framework.","keywords":""},{"title":"Jenkins Administration","type":0,"sectionRef":"#","url":"/docs/jenkins/administration","content":"","keywords":""},{"title":"Securityâ€‹","type":1,"pageTitle":"Jenkins Administration","url":"/docs/jenkins/administration#security","content":""},{"title":"LTS does not mean patchedâ€‹","type":1,"pageTitle":"Jenkins Administration","url":"/docs/jenkins/administration#lts-does-not-mean-patched","content":"Be aware that the Jenkins LTS release ships with default plugins which are not removable from the Jenkins instance and can be pinned to a low version, missing security updates. One such example is Jenkins LTS v2.176.2 which bundles the Ant plugin at version 1.2, released in 2013. There was a security advisory on 22nd Jan 2018 which identified an XSS vulnerability in this plugin affecting versions up to and including 1.7. At the time of writing this a year and a half later, the current LTS Jenkins is still bundling a version of this plugin from nearly 5 years prior. Many users will have no need for this plugin to be present, but it is not removable. I have to presume that the level of automated testing for Jenkins/CloudBees to perform even with less frequent LTS releases is prohibitive from patching all plugins even for security fixes. "},{"title":"Engineering Handbook","type":0,"sectionRef":"#","url":"/docs/engineering/engineering-handbook","content":"","keywords":""},{"title":"Onboarding levelsâ€‹","type":1,"pageTitle":"Engineering Handbook","url":"/docs/engineering/engineering-handbook#onboarding-levels","content":"Level 1 - How do we survive here? Basics, logging on, installing software, getting setup, lunch, holidays, timesheets, booking time off, benefits, pay and pension, etc.Level 2 - How do we operate? How do things get delivered, where does the work come from, what needs to be done, how do we do it, how do we deliver it downstream to our customers, how do we get feedback, what teams do I interface with, how do I contact them, who do I need to know, what do I need to know?Level 3 - How do I get my ideas done? What needs to happen to take my idea round the loop of getting something committed, in progress, delivered, feedback and iterate?Level 4 - How do I change the way we operate? Who are the decision-makers, what information would they need to make a decision which changes how we work? Level 1 - Worthless. If I can't get help on this fundamental level I am being fundamentally mis-treated. Some teams operate to provide services entirely at this level. The impact of what they do and how they do it should not be underestimated. It can undermine the higher levels. Level 2 - Cog. I am a cog in a machine, a potentially crucial but ultimately replaceable part. I have a clear function but no part to play in the organisation or value outsie of this limited function. Level 3 - Valued. I feel valued for my individual contribution, my ideas matter, people listen to me and I can get things done. It is worthwhile to enagage with this company. Level 4 - Empowered. I can make things better for everyone. I can change the system and not be beaten by it. These levels should not be sequential although they often are, and often stop before getting to level 3. Chris Hadfield had a dream from childhood of being an astronaut, yet he didn't feel complete even after he went to space for the first time, as he felt he hadn't contributed (from An astronaut's guide to life on earth, Pre-launch, The trip takes a lifetime). "},{"title":"Core principlesâ€‹","type":1,"pageTitle":"Engineering Handbook","url":"/docs/engineering/engineering-handbook#core-principles","content":"Psychological Safety, above ALL else. "},{"title":"THINKâ€‹","type":1,"pageTitle":"Engineering Handbook","url":"/docs/engineering/engineering-handbook#think","content":"Think before you speak. T - is it True?H - is it Helpful?I - is it Inspiring?N - is it Necessary?K - is it Kind? "},{"title":"Fix the next oneâ€‹","type":1,"pageTitle":"Engineering Handbook","url":"/docs/engineering/engineering-handbook#fix-the-next-one","content":"Don't start off trying to fix everything, or change things in flight. Build the machine that builds the machine, aim to fix the next one. "},{"title":"Solve the generic problemâ€‹","type":1,"pageTitle":"Engineering Handbook","url":"/docs/engineering/engineering-handbook#solve-the-generic-problem","content":"Resolving a specific problem or bug is mostly worthless, if the same class of problem will come up again. From experience at Rapha, fixing individual orders was far less important than resolving the underlying cause. Back in the day I would often forget my access pass when going to work. Resolving the issue wasn't about going to reception to get a temporary pass, it was about working out a system whereby I would not forget my access pass again. I made a habit of putting my pass in my work shoes whenever I got home, and I never forgot my pass again. This is closely related to the principle of staying on top of tech debt as a routine process, and optimising for the mid to long term. This is a counter to the anti-pattern of low hanging fruit, which may be easy but are they necessarily valuable? "},{"title":"Document appropriatelyâ€‹","type":1,"pageTitle":"Engineering Handbook","url":"/docs/engineering/engineering-handbook#document-appropriately","content":"Don't make engineers think about documentation more than necessary. Create templates for key doc types, which lead the author to creating useful content. Define a structure or tagging mechanism, so that any docs which need to be updated are easy to find. If it's not worth the time to keep any given document up-to-date, don't write it in the first place. See https://www.writethedocs.org/videos/eu/2017/the-four-kinds-of-documentation-and-why-you-need-to-understand-what-they-are-daniele-procida/ "},{"title":"Batch sizeâ€‹","type":1,"pageTitle":"Engineering Handbook","url":"/docs/engineering/engineering-handbook#batch-size","content":"There is a limit to any process, where at it's most efficient it has a maximum throughput. Always seek out ways to lower the cost of delivering valuable changes quickly, or &quot;better software, faster&quot; as Dave Farley puts it. No-one will complain about a build on a commit taking an hour or more when the cost of getting two code reviews and an external review takes two weeks. "},{"title":"Pull the Andon cordâ€‹","type":1,"pageTitle":"Engineering Handbook","url":"/docs/engineering/engineering-handbook#pull-the-andon-cord","content":"Please, please, please, pull the cord. It must be safe. It must be explicitly and repeatedly communicated in advance and by experience that pulling the cord is the not only ok but encouraged. It only takes one incident, one bad experience where someone is put in their place for doing something, that they will never do it again (at the same company). The cost of losing someone's trust in the safely of their environmnet is unaccptably high. "},{"title":"5 levels of delegationâ€‹","type":1,"pageTitle":"Engineering Handbook","url":"/docs/engineering/engineering-handbook#5-levels-of-delegation","content":"Level 1: Do as I say. This means to do exactly what I have asked you to do. Donâ€™t deviate from my instructions. I have already researched the options and determined what I want you to do.Level 2: Research and report. This means to research the topic, gather information, and report what you discover. We will discuss it, and then I will make the decision and tell you what I want you to do.Level 3: Research and recommend. This means to research the topic, outline the options, and bring your best recommendation. Give me the pros and cons of each option, then tell me what you think we should do. If I agree with your decision, I will authorize you to move forward.Level 4: Decide and inform. This means to make a decision and then tell me what you did. I trust you to do the research, make the best decision you can, and then keep me in the loop. I donâ€™t want to be surprised by someone else.Level 5: Act independently. This means to make whatever decision you think is best. No need to report back. I trust you completely. I know you will follow through. You have my full support. "},{"title":"Engineering Levelsâ€‹","type":1,"pageTitle":"Engineering Handbook","url":"/docs/engineering/engineering-handbook#engineering-levels","content":"Not an exhaustive list, more like general characteristics. Engineers should typically perform most activities at the right level. "},{"title":"Junior Engineerâ€‹","type":1,"pageTitle":"Engineering Handbook","url":"/docs/engineering/engineering-handbook#junior-engineer","content":"None of these are criticisms of junior engineers per say, everyone starts somewhere. This list is more driven by my experience with mid and seniors who do these things when they should know better. Needs to be shown what to do.Complains about security.Blocks tickets instead of following them up.Does not create documentation.Does not create diagrams.Scripts functionality instead of learning how to use toolsUnfamiliar with branching strategies.Leaves hanging branches in repositories.No opinions on how things should be done.Does not contribute to team discussions.Requires significant assistance with troubleshooting tasks.No knowledge or experience with any scripting languages or development tools. "},{"title":"Mid level Engineerâ€‹","type":1,"pageTitle":"Engineering Handbook","url":"/docs/engineering/engineering-handbook#mid-level-engineer","content":"The inverse of the things a junior doesn't/won't/can't do, plus: Will pro-actively learn. Self-starter.Owns tickets through to Done.Creates good documentation.Updates content created by others, demonstrating awareness of what is and isn;t correct. Owns it.Engages with other teams. Volunteers to own issues.Writes good code to fill in gaps in automation.Creates secure infrastructure.Understands H/A, DR, scalability, cost, resilience, 5 well architected pillars, etc.Good understanding of IaC tools.Confident in asking for help or saying they don't understand something. "},{"title":"Senior Engineerâ€‹","type":1,"pageTitle":"Engineering Handbook","url":"/docs/engineering/engineering-handbook#senior-engineer","content":"All of the Mid level points, plus: Can and does teach others.Can architect solutions with diagrams and documentation.Pro-actively takes on and fixes problems.Creates templates for others to use and follow.Identifies and follows up on organisational issues and things outside of their immediate team's domain.Engages with senior management and other teams. Develpos relationships and contacts.Writes frameworks to simplify coding for others. Lowers overall cognitive load.Implements secure patterns for others to follow. Creates TF/CDK modules, python/node/ts packages, GitHub Actions, etc. "},{"title":"Pipeline DSL Notes","type":0,"sectionRef":"#","url":"/docs/jenkins/PipelineDSL","content":"","keywords":""},{"title":"Groovy closuresâ€‹","type":1,"pageTitle":"Pipeline DSL Notes","url":"/docs/jenkins/PipelineDSL#groovy-closures","content":"There has been a long-standing bug with using closures like .each{} on collections, where the behaviour was incorrect and inconsistent. This gist demonstrates some of this behaviour. The bug has now been fixed in plugin:workflow-cps 2.33 although it's worth noting that there may still be other bugs like this which at the time of writing are still unresolved. "},{"title":"Script approvals from librariesâ€‹","type":1,"pageTitle":"Pipeline DSL Notes","url":"/docs/jenkins/PipelineDSL#script-approvals-from-libraries","content":"If you are a non-admin user writing Jenkins shared libraries, note that non-whitelisted methods will be blocked with an error indicating a script approval is required. Unfortunately library code does not raise a script approval for an admin to approve and the code in question has to be added to a Pipeline job where the methods are called directly from a Pipeline job using &quot;Pipeline script&quot; or &quot;Pipeline Script from SCM&quot;. To address this, a pattern I have adopted is to include a Groovy Jenkinsfile in the shared library repository called script-approvals.groovy which makes all the method calls requiring approval. It is then easy to create a Jenkins job which executes this pipeline, which can be repeatedly executed by the team responsible for approving scripts until the script runs to completion. This is useful in environments where there are new or multiple Jenkins masters which a library will be used on. "},{"title":"jengu","type":0,"sectionRef":"#","url":"/docs/jenkins/jengu","content":"","keywords":""},{"title":"A lightweight Jenkins Shared Library test frameworkâ€‹","type":1,"pageTitle":"jengu","url":"/docs/jenkins/jengu#a-lightweight-jenkins-shared-library-test-framework","content":"I spent nearly two years working with Jenkins and building CI/CD pipelines for hundreds of repos, on Jenkins instances with thousands of jobs. There were enough common development patterns, tools and process across the projects needing CI and CD to warrant extensive use of Jenkins DSL Shared Libraries, to reduce the repetition in pipeline implementations across projects. When developing any library to be consumed out of site of the authors, two things which are paramount are good documentation and comprehensive automated tests. Testing Jenkins Shared Library code is possible, but there are not many examples of how to do this. We were testing lots of Java code and knew Jenkins could consume and display a JUnit test report in XML, plus the applications we were testing had builds running on branches and PRs which we wanted to for the Jenkins libraries also. I wrote a lightweight test framework, which is part-way between unit testing and integration testing. I've continued to develop this and it's now open source on GitHub at https://github.com/agarthetiger/jengu "},{"title":"Jenkins Plugins","type":0,"sectionRef":"#","url":"/docs/jenkins/JenkinsPlugins","content":"","keywords":""},{"title":"Custom tool pluginâ€‹","type":1,"pageTitle":"Jenkins Plugins","url":"/docs/jenkins/JenkinsPlugins#custom-tool-plugin","content":"Not compatible with Pipelines until JENKINS-30680 is resolved. This has been open since September 2015. "},{"title":"Pipeline Utility Stepsâ€‹","type":1,"pageTitle":"Jenkins Plugins","url":"/docs/jenkins/JenkinsPlugins#pipeline-utility-steps","content":""},{"title":"findFilesâ€‹","type":1,"pageTitle":"Jenkins Plugins","url":"/docs/jenkins/JenkinsPlugins#findfiles","content":"Returns a FileWrapper object. name - (String) file name and extensionpath - (String) file path including file name and extension. Path does not include the workspace path. directory - (Boolean) True for a directory, otherwise Falselength - (long) Length (size) of the file in byteslastModified - (long) &quot;Installed&quot; must include restarting the Jenkins Master. I've had a plugin cause a conflict when installed, so I uninstalled it. The uninstall didn't resolve the conflict so I restarted to completely remove it. 12 hours later I finally got the Jenkins Master up and running again. I'd been tracking down all the recent changes working backwards to find that a plugin installed 8 months previously (before I joined the company of course) was causing the conflict. I'd overlooked the signs initially, not believing that a change made 8 months ago could have caused the problem. The master had not been restarted in 8 months. If the master had been restarted at the time, the problem would have been detected immediately and the change rolled back right away. Lesson learned, always restart Jenkins after installing or removing plugins and check the logs to ensure the instance comes up cleanly. â†© "},{"title":"Pipeline Steps","type":0,"sectionRef":"#","url":"/docs/jenkins/PipelineSteps","content":"","keywords":""},{"title":"shâ€‹","type":1,"pageTitle":"Pipeline Steps","url":"/docs/jenkins/PipelineSteps#sh","content":"The bash defaults for running sh script is set -xe which will print all commands and arguments when executed (-x) and terminate on any non-zero exit code (-e). Note that this behaviour changes if using returnStatus: true. In this case the sh step will not terminate on error but the build will still be marked as failed overall, but the build execution will not be halted. It is down to you to check the value returned by the sh step. "},{"title":"Quotes and shâ€‹","type":1,"pageTitle":"Pipeline Steps","url":"/docs/jenkins/PipelineSteps#quotes-and-sh","content":"Quotes in Jenkinsfiles are not always as simple to get right as you may think. Jenkinsfiles are groovy DSL, which often reference environment variables such as the values for parameterised jobs. Environment variables and groovy vars may both need to be passed to an echo step or to a sh step, sometimes to be evaluated in the Jenkins context, sometimes by the sh. An apparently simple task using process substitution to create a virtual .netrc file or creating a temporary json file (without using writeJSON) is often not quite so straightforward unless you remember the rules for quotations for each context.  "},{"title":"Debugging sh scriptsâ€‹","type":1,"pageTitle":"Pipeline Steps","url":"/docs/jenkins/PipelineSteps#debugging-sh-scripts","content":"Add 'cat -n \\$0' into sh scripts to get Jenkins to echo out the shell script and line numbers into the console log, to check what Jenkins is actually executing on the agent. From this blog "},{"title":"NonCPS annotationâ€‹","type":1,"pageTitle":"Pipeline Steps","url":"/docs/jenkins/PipelineSteps#noncps-annotation","content":"DSL code can only call methods or use classes which are serializable, unless the non-serializable methods are annotated with @NonCPS. Note there are additional considerations to be aware of. Methods annotated with @NonCPS cannot call any DSL methods, or any other methods unless the other methods are also annotated as NonCPS.Methods annotated with @NonCPS which call other methods will have undefined behaviour, including but not limited to returning unexpected values.Echo appears to be safe to use to log information from @NonCPS methods. "},{"title":"IUS","type":0,"sectionRef":"#","url":"/docs/linux/ius","content":"","keywords":""},{"title":"Consuming packages from IUSâ€‹","type":1,"pageTitle":"IUS","url":"/docs/linux/ius#consuming-packages-from-ius","content":"While there are setup and usage pages on the IUS website, there are still a couple of things to note when trying to find or install a package which are not well documented. "},{"title":"Package namesâ€‹","type":1,"pageTitle":"IUS","url":"/docs/linux/ius#package-names","content":"As per the safe replacement packages notes, IUS packages &quot;Uses a different name than the stock package to prevent unintended upgrades&quot;. This means that with the ius release repo enabled you may need to search for a wildcard name, like yum list git*, in order to find the package name to install. The IUS package will not be called just git as this would clash with the stock package name from yum. There is a package search link on the IUS site. This navigates to GitHub where you can see some of the packages available. Searching here for git yielded the repo iusrepo/git222. With the ius-release repo enabled, searching for packages matching git* also showed git2u which installs git v2.16.5-1. "},{"title":"Troubleshooting issues on Mac OS","type":0,"sectionRef":"#","url":"/docs/mac-os/troubleshooting","content":"","keywords":""},{"title":"Yarnâ€‹","type":1,"pageTitle":"Troubleshooting issues on Mac OS","url":"/docs/mac-os/troubleshooting#yarn","content":"Symptom: Running any yarn command yields the following error. $ yarn --version env: node: No such file or directory  Problem: Not sure, Stack Overflow indicates this could be a node package installed with Windows line endings, but I don't see how this could have happened. Running nvm or npm seems fine. I suspect this is linked to installing yarn with homebrew when a different node version was active via nvm, and now some path in yarn to where node lives has changed. I've had this with brew installed tools based on python and also brew installing python. When python gets updated via brew, the path to python changes and other tools, virtual environments etc all break. This is why I use pipx and don't rely on brew for anything related to python anymore, especially for python itself. Solution: Run the following commands to relink yarn. $ brew link --overwrite yarn Warning: Already linked: /opt/homebrew/Cellar/yarn/1.22.19 To relink, run: brew unlink yarn &amp;&amp; brew link yarn $ brew unlink yarn &amp;&amp; brew link yarn Unlinking /opt/homebrew/Cellar/yarn/1.22.19... 2 symlinks removed. Linking /opt/homebrew/Cellar/yarn/1.22.19... 2 symlinks created. $ which yarn /opt/homebrew/bin/yarn $ yarn -h # Normal yarn operation resumes here...  Follow ups: See whether switching node versions via nvm breaks yarn... "},{"title":"Glossary","type":0,"sectionRef":"#","url":"/docs/openshift/glossary","content":"","keywords":""},{"title":"Iâ€‹","type":1,"pageTitle":"Glossary","url":"/docs/openshift/glossary#i","content":"IPI - Installer Provisioned Infrastructure. This refers to whether the OpenShift installer provisions the underlying infrastructure or the user does this first. See UPI. "},{"title":"Uâ€‹","type":1,"pageTitle":"Glossary","url":"/docs/openshift/glossary#u","content":"UPI - User Provisioned Infrastructure "},{"title":"OpenShift Monitoring","type":0,"sectionRef":"#","url":"/docs/openshift/openshift-monitoring","content":"OpenShift Monitoring OpenShift ships with built-in monitoring for the cluster, which runs in the openshift-monitoring namespace. This includes Prometheus, AlertManager and Grafana. Openshift uses node-exporter to monitor the nodes in a cluster, which has a 15s scrape internal. This can be checked using the Route exposed for Prometheus on the /config endpoint. The scrape internal has implications on the appropriate time intervals used for the PromQL rate() and irate() functions.","keywords":""},{"title":"oc and Ansible k8s","type":0,"sectionRef":"#","url":"/docs/openshift/oc-and-ansible-k8s","content":"","keywords":""},{"title":"ocâ€‹","type":1,"pageTitle":"oc and Ansible k8s","url":"/docs/openshift/oc-and-ansible-k8s#oc","content":"RedHat provide several CLIs to interact with OpenShift. With OpenShift 4.4 these include oc - An operations-focussed tool for managing OpenShift clusters.odo - OpenShift Do, a developer-focussed tool for developing applications with OpenShift, intentionally abstracting away the complexities of OpenShift and Kubernetes. tkn - Tekton CLI for interacting with Tekton, the Kubernetes-native build tool behind OpenShift Pipelines. kn - Knative CLI for interacting with OpenShift Serverless. "},{"title":"Ansible k8sâ€‹","type":1,"pageTitle":"oc and Ansible k8s","url":"/docs/openshift/oc-and-ansible-k8s#ansible-k8s","content":"Ansible supports working with Kubernetes via the k8s modules. These were included with Ansible up to v2.9 but when v2.10 is released this will need to be installed using the community kubernetes collection. Either way, this provides a potentially idempotent way to manage resources in Kubernetes. I say potentially because it's still possible to unintentionally use these modules in a non-idempotent manner, more on that later. Ansible 2.7 through to 2.9 has seen additions to the k8s family of modules including k8s_auth, k8s_service and k8s_info, as well as enhancements like the addition of wait, wait_timeout and wait_condition to the k8s module. This Ansible blog post provides some more details. "},{"title":"Convert oc CLI commands to Ansible k8sâ€‹","type":1,"pageTitle":"oc and Ansible k8s","url":"/docs/openshift/oc-and-ansible-k8s#convert-oc-cli-commands-to-ansible-k8s","content":"It may not be immediately apparent what K8s resources need to be created to provide parity with oc commands. Here is a rough guide to what you'll need to create with k8s in order to replicate oc commands. Note as always that when writing or updating automation, simply automating exactly what came before is not always optimal especially if the process being automated was manual. oc new-app - DeploymentConfig, ReplicationController, ImageStream, BuildConfig, Service oc expose - Route oc secrets link - ServiceAccount update to add linked secret oc import-image - ImageStream oc new_build - BuildConfig "},{"title":"Podman and MacOS","type":0,"sectionRef":"#","url":"/docs/podman/podman-and-macos","content":"Podman and MacOS Podman is intended as a drop-in replacement for the docker cli and has from the outset had a focus on security and not running anything as root. You can run podman on MacOS using podman-machine instead of needing VirtualBox. The new MacBooks use the M1 or M2 chips, which are based on arm64 chips. This means there is a bit more work to do in order to build images for amd64 architecture. A bit more Googling that I expected turned up this handy thread on Reddit, which makes it easy to run multi-architecture builds on MacOS with Podman. podman machine ssh sudo -i rpm-ostree install qemu-user-static systemctl reboot After that you should be able to build different architectures by passing the --platform flag to podman. No need to install or run buildx.","keywords":""},{"title":"Puppet","type":0,"sectionRef":"#","url":"/docs/puppet/","content":"","keywords":""},{"title":"Puppet Agent command referenceâ€‹","type":1,"pageTitle":"Puppet","url":"/docs/puppet/#puppet-agent-command-reference","content":"cat $(puppet agent --configprint resourcefile) - List all resources managed by puppet.cat $(puppet agent --configprint agent_disabled_lockfile) - Check whether the puppet agent is disabled and why.puppet agent --disable &quot;Who disabled puppet agent, when and why.&quot; - Disable the puppet agent.cat /opt/puppetlabs/puppet/cache/state/classes.txt - List all classes applied to the current node. Note the file location may be configured differently.puppet agent --test --noop - Run the puppet agent once in noop mode without starting the daemon, to check what would be applied. "},{"title":"OpenTelemetry with AWS Lambda","type":0,"sectionRef":"#","url":"/docs/opentelemetry/otel-and-aws-lambda","content":"","keywords":""},{"title":"Resourcesâ€‹","type":1,"pageTitle":"OpenTelemetry with AWS Lambda","url":"/docs/opentelemetry/otel-and-aws-lambda#resources","content":"AWS Distribution for Open Telemetry (ADOT) Note that also here you need to instrument undici if using fetch in the OTEL_NODE_ENABLED_INSTRUMENTATIONS. OTel Bin - Validate your collector.yaml configuration against known distributions. I raised a request many months ago but surprisingly you still can't validate configuration against the official OpenTelemetry Lambda Layers.Instrumentation Score - A project to objectively &quot;score&quot; your instrumentation.Blog - Instrumenting a Typescript Lambda with HoneycombHoneycomb - Export Data with AWS Lambda Layer + OpenTelemetry "},{"title":"Using Bluetooth with Python","type":0,"sectionRef":"#","url":"/docs/python/bluetooth","content":"","keywords":""},{"title":"Glossaryâ€‹","type":1,"pageTitle":"Using Bluetooth with Python","url":"/docs/python/bluetooth#glossary","content":"Central - Generally a more powerful device than a peripheral, like a mobile phone or Raspberry Pi Zero W. A Central device is a GATT Client. GAP - Generic Access Profile - GAP controls connection and advertising with bluetooth. GAP makes your device visible and determines how devices (central) can interact with it. GATT - Generic Attribute Profile - GATT defines the way paired devices communicate, using Services and Characteristics. It makes use of a generic protocol called the Attribute Protocol (ATT) which is used to store service, Characteristics and related data in a simple lookup using 16-bit IDs for each entry in the table. Peripheral - A low power device which can advertise itself and be connected to by a Central. Sensors like heart rate straps and power meters are peripherals. A peripheral is a GATT Server. "},{"title":"Notesâ€‹","type":1,"pageTitle":"Using Bluetooth with Python","url":"/docs/python/bluetooth#notes","content":""},{"title":"GAPâ€‹","type":1,"pageTitle":"Using Bluetooth with Python","url":"/docs/python/bluetooth#gap","content":"Advertising packets are sent out by a peripheral device regularly. A Central can request a scan response request which the peripheral can optionally respond to with a slightly more detailed message (which is limited to 31 bytes, the same size as the advertising data message). A beacon can only broadcast data to multiple devices by not pairing with any one device. Once paired, only the paired device can communicate with the peripheral and advertising messages will stop. "},{"title":"GATTâ€‹","type":1,"pageTitle":"Using Bluetooth with Python","url":"/docs/python/bluetooth#gatt","content":"A GATT Server (peripheral) holds the ATT lookup data, service and chacteristic definitions. The GATT Client (Central device) sends requests to the server. All transactions are initiated by the Client/Central. When establishing a connection, the Server/Peripheral will suggest a &quot;Connection Interval&quot; to the Client/Central and the central should try and reconnect on this interval to check for new data. This is just a suggestion, as the Client/Central device may not be busy and not always able to try and reconnect on this interval. "},{"title":"Profiles, Services and Characteristicsâ€‹","type":1,"pageTitle":"Using Bluetooth with Python","url":"/docs/python/bluetooth#profiles-services-and-characteristics","content":"Profiles are collections of Services and Characteristics. See https://www.bluetooth.com/specifications/gatt/ for the official specifications for Profiles and Services. A Service contains one or more Characteristics and are used to group Characteristics into logical entities. Each Service has a UUID which is either 16-bit for officially adopted BLE Service or 128-bit for custom devices. The UUID for the Cycling Power Service is 0x1818 Characteristics encapsulate a single data point, thoug it may contain an array of related data such as X,Y,Z co-ordinates. Like Services, each Characteristic has a 16-bit UUID for or 128-bit. Seehttps://btprodspecificationrefs.blob.core.windows.net/assigned-values/16-bit%20UUID%20Numbers%20Document.pdf for a list of these (from https://www.bluetooth.com/specifications/assigned-numbers/). The UUID for Cycling Power Measurement Characteristic is 0x2A63 "},{"title":"Company IDsâ€‹","type":1,"pageTitle":"Using Bluetooth with Python","url":"/docs/python/bluetooth#company-ids","content":"Companies can apply for their own official IDs. Look them up on https://www.bluetooth.com/specifications/assigned-numbers/company-identifiers/ Favero - 0x0364 (Decimal 868)Garmin - 0x0087 (Decimal 135)Team Zwatt - Sensitivus - Unknown (should be able to get this from my device though when I try and connect to it) "},{"title":"Cycling Power Measurement Characteristicâ€‹","type":1,"pageTitle":"Using Bluetooth with Python","url":"/docs/python/bluetooth#cycling-power-measurement-characteristic","content":"All Characteristics used with this service shall be transmitted with the least significant octet first (ie. Little endian). The server should notify this characteristic at a regluar interval, typically once per second while in a connection and is not configurable by the client. If the LE server supports broadcast, this Characteristic may be available in the broadcast message (although unlikely given how you don't want people around you to pick up your data). As this Characteristic does not contain a time-stamp and is time-sensitive data, it will be discarded if not successfully transmitted.  "},{"title":"Codingâ€‹","type":1,"pageTitle":"Using Bluetooth with Python","url":"/docs/python/bluetooth#coding","content":"AdaFruit have written an &quot;AdaFruit CircuitPython BLE&quot; library, which builds on top of the AdaFruit Blinka bleio library, which is based on bleak. "},{"title":"BLE libraryâ€‹","type":1,"pageTitle":"Using Bluetooth with Python","url":"/docs/python/bluetooth#ble-library","content":"GitHubRead The Docs "},{"title":"Bleioâ€‹","type":1,"pageTitle":"Using Bluetooth with Python","url":"/docs/python/bluetooth#bleio","content":"GitHub "},{"title":"Bleakâ€‹","type":1,"pageTitle":"Using Bluetooth with Python","url":"/docs/python/bluetooth#bleak","content":"Bluetooth Low Energy platform Agnostic Klient for python GitHub "},{"title":"Pycyclingâ€‹","type":1,"pageTitle":"Using Bluetooth with Python","url":"/docs/python/bluetooth#pycycling","content":"See this pypi.org lib/GitHub repo for an implementation which looks very promising. There are examples of scanning for devices and potentially reading the data I need. Has some examples, a bit light on documentation but well worth starting from here rather than the raw BLE AdaFruit libraries. "},{"title":"Referencesâ€‹","type":1,"pageTitle":"Using Bluetooth with Python","url":"/docs/python/bluetooth#references","content":"Adafruit bluetooth tutorial - https://learn.adafruit.com/introduction-to-bluetooth-low-energy/introduction See tutorial for GAP and GATT information. GATT XML specifications - https://github.com/sur5r/gatt-xmlPython package for interacting with BLE trainers and power meters. - https://github.com/zacharyedwardbull/pycycling "},{"title":"Interesting projectsâ€‹","type":1,"pageTitle":"Using Bluetooth with Python","url":"/docs/python/bluetooth#interesting-projects","content":"Raspberry Pi Zero W based bike computer using python (Note no bluetooth though, power and speed via Ant+, but cool nonetheless) - https://github.com/hishizuka/pizero_bikecomputer "},{"title":"Python Resources","type":0,"sectionRef":"#","url":"/docs/python/resources","content":"Python Resources Using Python with Docker - pythonspeed.comJetBrains Python Developers Survey 2019","keywords":""},{"title":"MKDocs","type":0,"sectionRef":"#","url":"/docs/python/mkdocs","content":"","keywords":""},{"title":"Travis CI Enhancementsâ€‹","type":1,"pageTitle":"MKDocs","url":"/docs/python/mkdocs#travis-ci-enhancements","content":"My yaml file looks like this. language: python branches: only: - master git: depth: 1 install: - pip install mkdocs - pip install mkdocs-material script: - if [[ $TRAVIS_PULL_REQUEST == &quot;false&quot; ]]; then mkdocs build --strict; fi; deploy: skip_cleanup: true provider: script script: bash travis-ci/deploy-to-gh-pages.sh on: branch: master  I kept the --depth option to still shallow clone the repo with a single branch for the build. Using the branches whitelist I could remove the requirement to check the branch name later in the script, and using a deploy phase meant no additional logic is required to check for pull requests or the branch in the shell script travis-ci/deploy-to-gh-pages.sh. deploy.skip_cleanup stops Travis from stashing changes in the workspace between the (build) script and deploy phases. "},{"title":"Blinkt!","type":0,"sectionRef":"#","url":"/docs/raspberry-pi/blinkt","content":"","keywords":""},{"title":"Controlling Blinkt!â€‹","type":1,"pageTitle":"Blinkt!","url":"/docs/raspberry-pi/blinkt#controlling-blinkt","content":"This getting started guide is a great place to start if you already know a bit of python and want to start working with your Blinkt!. "},{"title":"AdaFruit miniPiTFT","type":0,"sectionRef":"#","url":"/docs/raspberry-pi/adafruit-mini-pi-tft","content":"AdaFruit miniPiTFT AdaFruit make several small displays for Raspberry Pi, including some tiny TFT displays. I bought a Raspberry Pi Zero to use as a Pi Hole to block adverts on my home network and also bought an Adafruit Mini PiTFT - 135x240 Color TFT Add-on for Raspberry Pi, mostly because it looked cool! It has the added benefit of allowing me to check the Pi is up and running if I have any network issues when checking the physical networking around my router. I have no intention of displaying the console output to this device, so opted to install the python drivers for this display instead of the kernel drivers which cannot both be installed at the same time. The display can be controlled using CircuitPython. Getting the display running was straight-forward if a little manual following the AdaFruit guide. I want to make something a bit more advanced, so I went down an interesting yet unfortunate rabbit hole learning about displayio, Groups, TileGrids etc only to find out that there is no support for displayio on Raspberry Pi. Instead this display needs to use the AdaFruit CircuitPython RGB Display python library and Pillow. The AdaFruit GitHub repo has some useful examples, as does learn.adafruit.com.","keywords":""},{"title":"Python Packages","type":0,"sectionRef":"#","url":"/docs/python/packages","content":"","keywords":""},{"title":"CLI toolsâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#cli-tools","content":""},{"title":"Clickâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#click","content":"https://click.palletsprojects.com/en/7.x/ Definitely my favourite CLI tool so far. Read a comparison of argparse, docopt and click on Real Python "},{"title":"Nubiaâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#nubia","content":"https://github.com/facebookincubator/python-nubia/blob/master/README.md Nubia is a lightweight framework for building command-line applications with Python. It was originally designed for the &quot;logdevice interactive shell (aka.Â ldshell)&quot; at Facebook. Since then it was factored out to be a reusable component and several internal Facebook projects now rely on it as a quick and easy way to get an intuitive shell/cli application without too much boilerplate. Nubia is built on top ofÂ python-prompt-toolkitÂ which is a fantastic toolkit for building interactive command-line applications. "},{"title":"Richâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#rich","content":"https://rich.readthedocs.io/en/latest/ Rich is a Python library for writing rich text (with color and style) to the terminal, and for displaying advanced content such as tables, markdown, and syntax highlighted code. I'll use this in the next release of hint. "},{"title":"Documentationâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#documentation","content":""},{"title":"Portrayâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#portray","content":"https://github.com/timothycrosley/portray "},{"title":"MKDocsâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#mkdocs","content":"https://www.mkdocs.org/ "},{"title":"pdoc3â€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#pdoc3","content":"https://pdoc3.github.io/pdoc/ "},{"title":"terminalizerâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#terminalizer","content":"https://terminalizer.com/ Ok, so it's Node.js but useful to note here to generate animated terminal gifs for cli documentation. "},{"title":"Generalâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#general","content":""},{"title":"isortâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#isort","content":"https://github.com/timothycrosley/isort Sorts your imports, so you don't have to. "},{"title":"natsortâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#natsort","content":"https://natsort.readthedocs.io/en/master/ Sort things 'naturally', including but not limited to semantic version numbers. "},{"title":"parseâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#parse","content":"https://pypi.org/project/parse/ Simplified regex-based string search. "},{"title":"python-benedictâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#python-benedict","content":"https://github.com/fabiocaccamo/python-benedict dict subclass with keylist/keypath support, I/O shortcuts (base64, csv, json, pickle, plist, query-string, toml, xml, yaml) and many utilities. "},{"title":"Diagrams as codeâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#diagrams-as-code","content":"https://diagrams.mingrammer.com/ "},{"title":"GUI Toolsâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#gui-tools","content":""},{"title":"Dear PyGUIâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#dear-pygui","content":"https://github.com/hoffstadt/DearPyGui "},{"title":"Logging and error handlingâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#logging-and-error-handling","content":""},{"title":"PrettyErrorsâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#prettyerrors","content":"https://github.com/onelivesleft/PrettyErrors &quot;Prettifies Python exception output to make it legible&quot;. Tracebacks are legible IMO, the problems are mostly getting users (including myself) to read them carefully and diligently every time. They could still be formatted better for humans though. One to review, my initial impression is I'm not 100% sold on the output. One to compare and constrast with the logging and error handling features in Rich. Could be a useful tool for new users of python though, as it can be configured with no code changes required. "},{"title":"Managingâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#managing","content":""},{"title":"Poetryâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#poetry","content":"https://python-poetry.org/ Packaging and dependency management. Doesn't yet support autoversioning from Git tags, that's dependent on the plugin framework in v1.2, hopefully soon. Still great for personal projects where manually bumping versions is manageable (ymmv) and releasing to pypi.org is fine to do locally. "},{"title":"Sailboatâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#sailboat","content":"https://github.com/cole-wilson/sailboat "},{"title":"Templatingâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#templating","content":""},{"title":"Cruftâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#cruft","content":"https://timothycrosley.github.io/cruft/ Built in top of cookiecutter, Cruft enables a project template to be reapplied as and when the template is updated. "},{"title":"Versioningâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#versioning","content":""},{"title":"setuptools-scmâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#setuptools-scm","content":"https://pypi.org/project/setuptools-scm/ setuptools_scm handles managing your Python package versions in SCM metadata instead of declaring them as the version argument or in a SCM managed file. It also handles file finders for the supported SCMs. "},{"title":"Zest-Releaserâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#zest-releaser","content":"https://github.com/zestsoftware/zest.releaser Versions using one of four different files, no support for using git tags :( "},{"title":"bumpversionâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#bumpversion","content":"https://pypi.org/project/bumpversion/ "},{"title":"Python Semantic Releaseâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#python-semantic-release","content":"https://github.com/relekang/python-semantic-release "},{"title":"Webâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#web","content":""},{"title":"BeautifulSoupâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#beautifulsoup","content":"https://www.crummy.com/software/BeautifulSoup/bs4/doc/ Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. Use with Requests. "},{"title":"Scrapyâ€‹","type":1,"pageTitle":"Python Packages","url":"/docs/python/packages#scrapy","content":"https://doc.scrapy.org/en/latest/index.html Scrapy is a fast (async) high-level web crawling and web scraping framework, used to crawl websites and extract structured data from their pages. It can be used for a wide range of purposes, from data mining to monitoring and automated testing. "},{"title":"Raspberry Pico W","type":0,"sectionRef":"#","url":"/docs/raspberry-pi/pico/","content":"","keywords":""},{"title":"Getting started with a Raspberry Picoâ€‹","type":1,"pageTitle":"Raspberry Pico W","url":"/docs/raspberry-pi/pico/#getting-started-with-a-raspberry-pico","content":"Hold down BOOTSEL button when connecting to usb to enable mass storage device. Use this to transfer firmware (.uf2 files) like MicroPython onto the Pico. The Pico will load the formware and reboot, disconnecting from the pc and booting into whatever firmware was loaded. Use minicom to connect from the command line, minicom -o -D /dev/ttyACM0 althugh Thonny is much easier to use if you have a GUI. Select MicroPython (Raspberry Pi Pico) from the bottom-left dropdown and you get both a REPL (Read, Execute, Print, Loop) prompt and the ability to save and run code from an editor. "},{"title":"Linksâ€‹","type":1,"pageTitle":"Raspberry Pico W","url":"/docs/raspberry-pi/pico/#links","content":"The MicroPython SDK manualConnecting to the internet / WiFi "},{"title":"I2C on the Raspberry Pi Pico W","type":0,"sectionRef":"#","url":"/docs/raspberry-pi/pico/i2c","content":"","keywords":""},{"title":"IC2 master and slavesâ€‹","type":1,"pageTitle":"I2C on the Raspberry Pi Pico W","url":"/docs/raspberry-pi/pico/i2c#ic2-master-and-slaves","content":"Unfortunate terminology these days, so I'm going to call these types of device controllers and workers. A device with a controller can communicate with multiple worker devices, which will share the same clock and data connections. Each worker device has a fixed address, and all addresses must be unique for a controller. The Raspberry Pi Pico W has two I2C controllers. https://www.raspberrypi.com/documentation/microcontrollers/images/picow-pinout.svg To be continued... "},{"title":"Referencesâ€‹","type":1,"pageTitle":"I2C on the Raspberry Pi Pico W","url":"/docs/raspberry-pi/pico/i2c#references","content":"https://www.hackster.io/mr-alam/how-to-use-i2c-pins-in-raspberry-pi-pico-i2c-scanner-code-8f489f "},{"title":"Boiler Pico","type":0,"sectionRef":"#","url":"/docs/raspberry-pi/pico/boiler-pico","content":"","keywords":""},{"title":"Hardwareâ€‹","type":1,"pageTitle":"Boiler Pico","url":"/docs/raspberry-pi/pico/boiler-pico#hardware","content":"For projects where I might have used a Pi to run a single python script a Pico may be more suitable. There is no OS to patch with security updates, I can run a micropython script with much faster startup time and the Pico W has WiFi 2.4GHz for wireless communication. Primarily I started looking at these because they were both cheap and available. I'm sure there are alternatives which have been around for a long time. However, I have two kids and very limited free time. I find the community around Raspberry Pis to be a great help, probably everything I want to do has already been done before, problems encountered and solved. At this point in my life this is a significant bonus. I have a spare Raspberry Pi Zero W from another project which I could repurpose for this project, but the more I think about it the more a single purpose microcontroller like a Pico is better suited to this job. In searching around for suitable temperature sensors the DS18B20 came up. I'm hoping these will prove suitable with a bit of thermal paste and insulation around the pipes and sensors. As long as the temperature readings are repeatable, I'd probably be ok with plus or minus a few degrees. I can see what the boiler says it's outputting for the water temperature which will be interesting to compare once this is built. I2C, SPI and one-wire are all quite new to me, so it was good to find tutorials and guides explaining each of them and even a guide for interfacing multiple DS18B20 devices with a Pico, which is how I intend to build this system. "},{"title":"Softwareâ€‹","type":1,"pageTitle":"Boiler Pico","url":"/docs/raspberry-pi/pico/boiler-pico#software","content":"I've been building up micropython scripts to get familiar with using the Pico. From another Raspberry Pi 400, using Thonny is a great way to get started. I used to be a full-time software developer, using tools like VS Code and IntelliJ PyCharm daily. Compared to my experiences with MacOS and Arduino devices in the past which were decidedly painful, the Pi400 and Thonny are such a simple combination and do everything I need. Starting with the Raspberry Pi Pico official guide, flashing micropython onto the Pico was really well explained. No recompiling kernels or anything like that, it was literally drag-and-drop onto the Pico. It's impossible to brick a Pico, the bootloader firmware is in read-only memory so with a press of a hardware button you can always reflash a Pico. I ran through several tutorials starting from flashing the onboard LED to connecting to WiFi, reading the onboard package temperature and driving a small oled display. Moving on, I also found guides for setting up a basic webserver, and serving metrics in the OpenMetrics format which Prometheus can scrape. Feels like all the pieces are in place to begin to build this up. Problems can always crop up, but in theory this should all work together. Time to build the thing. "},{"title":"Buildâ€‹","type":1,"pageTitle":"Boiler Pico","url":"/docs/raspberry-pi/pico/boiler-pico#build","content":"Hopefully by the end of this weekend there will be something physical to show... tbc. From left to right, bytearray addresses as mounted, b'($\\xb8Sa&quot;\\x07\\x88'), hex 28u24ub8u53u61u22u07u88b'(\\xe0#Ja&quot;\\x07\\xa7'), hex 28ue0u23u4au61u22u07ua7b'(\\xe2\\x81~T&quot;\\x08\\x95'), hex 28ue2u81u7eu54u22u08u95b'(\\xf7\\xa0sT&quot;\\x08\\x8d'), hex 28uf7ua0u73u54u22u08u8db'(JPoT&quot;\\x08L'), hex 28u4au50u6fu54u22u08u4c Momentary switch leds, Vf 2.2v. 3.3v - 2.2v = 1.1v 1.1v / 25mA == 1.1 / 0.025 = 44 ohm resistor required. ... but Pi Hut said for this illuminated switch &quot;220 to 1000 ohm resistor in series just as you would with any other LED to your 3V or higher power supply&quot;. Split the difference, 100 ohms looks fine, because splitting the difference is how electronics works... right? Joking aside, this worked but was felt overdriven so swapped for a 470 ohm instead which is a much more reasonable brightness. If only the product datasheet gave the recommended current for this button/led. "},{"title":"Referencesâ€‹","type":1,"pageTitle":"Boiler Pico","url":"/docs/raspberry-pi/pico/boiler-pico#references","content":"Thanks to the open source community who've made this project far more accessible to me by doing much of the heavy lifting and sharing. "},{"title":"Getting startedâ€‹","type":1,"pageTitle":"Boiler Pico","url":"/docs/raspberry-pi/pico/boiler-pico#getting-started","content":"https://www.raspberrypi.com/documentation/microcontrollers/raspberry-pi-pico.html "},{"title":"Reading sensorsâ€‹","type":1,"pageTitle":"Boiler Pico","url":"/docs/raspberry-pi/pico/boiler-pico#reading-sensors","content":"https://how2electronics.com/read-temperature-sensor-value-from-raspberry-pi-pico/https://how2electronics.com/interfacing-ds18b20-sensor-with-raspberry-pi-pico/https://docs.micropython.org/en/latest/esp8266/tutorial/onewire.htmlhttps://docs.micropython.org/en/v1.19.1/rp2/quickref.html#onewire-driverhttps://mpython.readthedocs.io/en/master/library/mPython/ds18x20.htmlhttps://forum.micropython.org/viewtopic.php?t=2914 "},{"title":"Exporting metrics for Prometheusâ€‹","type":1,"pageTitle":"Boiler Pico","url":"/docs/raspberry-pi/pico/boiler-pico#exporting-metrics-for-prometheus","content":"https://github.com/leszekuchacz/KitronikBME688-picow-exporterhttps://sysdig.com/blog/prometheus-metrics/https://github.com/OpenObservability/OpenMetrics/blob/main/specification/OpenMetrics.mdhttp://www.d3noob.org/2022/10/using-raspberry-pi-pico-with-prometheus.html "},{"title":"Electronics and hardware assembblyâ€‹","type":1,"pageTitle":"Boiler Pico","url":"/docs/raspberry-pi/pico/boiler-pico#electronics-and-hardware-assembbly","content":"https://www.evilmadscientist.com/2012/resistors-for-leds/https://proto-pic.co.uk/product/crimp-connector-housings-2-54mm-0-1-dupont-compatible "},{"title":"Reading WiFi signal strengthâ€‹","type":1,"pageTitle":"Boiler Pico","url":"/docs/raspberry-pi/pico/boiler-pico#reading-wifi-signal-strength","content":"https://forums.raspberrypi.com/viewtopic.php?t=341577 "},{"title":"Micropython referencesâ€‹","type":1,"pageTitle":"Boiler Pico","url":"/docs/raspberry-pi/pico/boiler-pico#micropython-references","content":"https://forums.raspberrypi.com/viewtopic.php?t=310062https://awesome-micropython.com/ "},{"title":"Pico W - now with added WiFi","type":0,"sectionRef":"#","url":"/docs/raspberry-pi/pico/pico-wifi","content":"","keywords":""},{"title":"Documentationâ€‹","type":1,"pageTitle":"Pico W - now with added WiFi","url":"/docs/raspberry-pi/pico/pico-wifi#documentation","content":"https://datasheets.raspberrypi.com/picow/connecting-to-the-internet-with-pico-w.pdfhttps://docs.micropython.org/en/latest/library/network.WLAN.htmlMicroPython GitHub https://github.com/micropython/micropython "},{"title":"Using a Pico as a temperature and humidity sensor","type":0,"sectionRef":"#","url":"/docs/raspberry-pi/pico/pico-sensor","content":"","keywords":""},{"title":"Hardwareâ€‹","type":1,"pageTitle":"Using a Pico as a temperature and humidity sensor","url":"/docs/raspberry-pi/pico/pico-sensor#hardware","content":"BME280 sensors from Ali Express, 3.3v variant. They are cheap, presumably also because they have no voltage regulator so they will probably break if connected to 5v. I2C or SPI but I'll use I2C on the same pins as the OLED display. White OLED displays from Ali Express Driver IC: SSD1306 Size: 0.91 inch OLED Resolution: 128 x 32 I2C interface Pin Description: GND: Power Ground VCC: Power + (DC 3.3 ~5v) SCL: Clock Line SDA: Data Line Links "},{"title":"BME280â€‹","type":1,"pageTitle":"Using a Pico as a temperature and humidity sensor","url":"/docs/raspberry-pi/pico/pico-sensor#bme280","content":"https://how2electronics.com/read-temperature-sensor-value-from-raspberry-pi-pico/https://microcontrollerslab.com/bme280-raspberry-pi-pico-micropython-tutorial/https://www.raspberrypi.com/documentation/microcontrollers/raspberry-pi-pico.html#pinout-and-design-files15 "},{"title":"Cluster resources","type":0,"sectionRef":"#","url":"/docs/raspberry-pi/raspberry-pi-cluster/resources","content":"","keywords":""},{"title":"PWM Fan Controlâ€‹","type":1,"pageTitle":"Cluster resources","url":"/docs/raspberry-pi/raspberry-pi-cluster/resources#pwm-fan-control","content":"Fancy - Python PWM fan control serviceGPIO and PWM in Raspberry Pi "},{"title":"Raspbian commands","type":0,"sectionRef":"#","url":"/docs/raspberry-pi/raspbian-commands","content":"","keywords":""},{"title":"vcgencmdâ€‹","type":1,"pageTitle":"Raspbian commands","url":"/docs/raspberry-pi/raspbian-commands#vcgencmd","content":"See vcgencmd documentation. vcgencmd commands - List available vcgencmd commands. Note there is no tab completion by default, nor a --help or equivalent option.vcgencmd measure_temp - Returns the temperature as measured by the SoC internal temperature sensor. Output eg. temp=42.2'C "},{"title":"wpa_cliâ€‹","type":1,"pageTitle":"Raspbian commands","url":"/docs/raspberry-pi/raspbian-commands#wpa_cli","content":"wpa_cli is a tool to configure and troubleshoot WiFi connectivity on a Raspberry Pi. "},{"title":"Part 1 - Initial design","type":0,"sectionRef":"#","url":"/docs/raspberry-pi/raspberry-pi-cluster/part-1","content":"","keywords":""},{"title":"Powerâ€‹","type":1,"pageTitle":"Part 1 - Initial design","url":"/docs/raspberry-pi/raspberry-pi-cluster/part-1#power","content":"I'll be starting out with a TP-Link TL-SG1008P 8 port gigabit ethernet switch with 4 ports capable of delivering Power over Ethernet (PoE). I considered PoE which does make for a clean build with minimal cables. Looking at the official Pi PoE board though there seems to be potential trouble lurking. While the PoE HAT is compatible with the Raspberry Pi 4 as well as the Pi 3 boards which were the latest at the time, the maximum current draw for a Pi 4 has gone up to 3A. Admittedly this is with additional USB peripherals and a peak load like at boot, but I don't want problems down the line with voltage drops. The PoE board can only deliver 2.5A max. Also my choice of switch delivers 55Watts max via PoE. Split this across 4 devices with an optimistic 80% efficiency in the PoE HAT and we're down to 2.2A max at 5V. The PoE HAT also generates heat right where we don't want it, right next to the Pi and it while it has a fan it looks to be a snug fit to the Pi board meaning heatsink on the Pi are probably also out of the question. While I've not decided on my final power soltuion, PoE is out for now. "},{"title":"Caseâ€‹","type":1,"pageTitle":"Part 1 - Initial design","url":"/docs/raspberry-pi/raspberry-pi-cluster/part-1#case","content":"I know I'll need an enclosure which can survive being moved between my house and garage and get by in a moderately dusty environment. The open stackable Pi cluster cases didn't have enough environmental protection for my liking. A pretty good candidate for a cluster case would actually be a gaming PC case. A PC PSU could potentially provide 5v power for the Pis as well as 5/12v for fans and a windows to view the LEDs which I will of course be fitting to each device, because bling! Cases can also provide convenient hard-drive mounting too which would be a tidy setup. I don't want this cluster to be a huge power draw though if left powered on but idle. Often the PSUs don't have great efficiency and are optimised for a clean single rail 12v supply which would be wasted just for the fans which I don't want running all the time anyway. I could use a dedicated passively cooled high current 5v supply, but now there's less value in using the PC case. A shop bought PC case also isn't very imaginative or unique so I'm leaning towards a home-made enclosure. I have a Sienci Mill One CNC machine from their original Kickstarter campaign so I should be able to make something tidy. "},{"title":"Coolingâ€‹","type":1,"pageTitle":"Part 1 - Initial design","url":"/docs/raspberry-pi/raspberry-pi-cluster/part-1#cooling","content":"One thing I expect this cluster will need is active cooling. The cluster is based around four Pi 4Bs which even with the updates since launch will still throttle under prolonged high cpu load. This cluster has to work in my garage as well as inside my house, so needs protection from dust which means more enclosure than the cheap cluster cases provide. Enclosure is bad for cooling but as this will also sit next to my desk indoors this also needs to be quiet and only activate the fans when cooling is required. After much deliberation my first build will use a single Noctua 120mm fan to provide airflow across all the hosts in the cluster. The cheap cluster cases which include fans are connected to the 3.3v or 5v power GPIO pins and are therefore always on. This fan can be controlled by a Pulse Width Modulation (PWM) signal from the nominated cluster controller which will be monitoring the temperature of all the hosts in the cluster. The NF-A12x25 in particular will stop when the fan is set to 0% PWM duty cycle by the PWM fan controller. This makes it ideal for creating setups with semi-passive cooling that automatically turn the fans off and thus run completely silent at idle if thermals allow. In the fan specs it needs a 5v PWM signal and the Pi's GPIO pins are 3.3v, so I'll be using a SparkFun Logic Level Converter to drive the fan. From the specs sheet the rise and fall response times should easily be adequate for a 100kHz PWM signal which is more than enough for the fan. "},{"title":"Rspberry Pi 400","type":0,"sectionRef":"#","url":"/docs/raspberry-pi/rpi400","content":"","keywords":""},{"title":"Notes on configurationâ€‹","type":1,"pageTitle":"Rspberry Pi 400","url":"/docs/raspberry-pi/rpi400#notes-on-configuration","content":"In lieu of having this automated, here is a list of stuff I've configured so far and some thoughts on what is stil to do. "},{"title":"GitHubâ€‹","type":1,"pageTitle":"Rspberry Pi 400","url":"/docs/raspberry-pi/rpi400#github","content":"Install the gh cli to authenticate over https via browser login, rather than ssh keys or tokens. See the docs for installation then run gh auth login to authenticate. "},{"title":"Other softwareâ€‹","type":1,"pageTitle":"Rspberry Pi 400","url":"/docs/raspberry-pi/rpi400#other-software","content":"Install minicom via apt, but not really using this anymore as Thonny is far better for talking to Picos. Install 1Password extension into Chromium. Still want to troubleshoot the standalone debian installer as this downloaded but failed to run. "},{"title":"To Doâ€‹","type":1,"pageTitle":"Rspberry Pi 400","url":"/docs/raspberry-pi/rpi400#to-do","content":"Things it would be nice to do with my RPi 400, Automate the setup, or at least schedule a MicroSD backup so I can recover.Boot from USB SSD drive instead of MicroSD, like all my other Pi 4B devices.Overclock, as the integrated heat spreader is decent and overclocking is considered pretty stable to a point.Note down my troubleshooting steps, as a few times I've struggled to get HDMI output from this onto my 4K monitor.... "},{"title":"Learn Tekton","type":0,"sectionRef":"#","url":"/docs/tekton/learn-tekton","content":"Learn Tekton If you are new to Tekton there are some great tutorials to get you started. IBM Tekton tutorialRed Hat Scholars Tekton TutorialOpenShift Pipelines Tutorial !!! Note &quot;Always check the version&quot; Remember to check what version of Tekton is installed in your environment vs what version the tutorial or documentation is based on. The Red Hat Scholars Tekton Tutorial has a Setup page which calls out the version of Tekton, the Tekton CLI and all other tools which the tutorial is based on. !!! Warning &quot;Here be dragons&quot; Note that at time of writing, some CRDs in Tekton are in Beta release while some are still in Tech Preview. Things may change in a breaking way. There is also a Tekton docs site. Note though that this only shows documentation for the latest version of Tekton, and the links in the top right nav to other versions just send you to GitHub. I hope that once Tekton has a GA release under it's belt the docs will be versioned as per any other enterprise-grade tool.","keywords":""},{"title":"Part 2 - Smart cooling","type":0,"sectionRef":"#","url":"/docs/raspberry-pi/raspberry-pi-cluster/part-2","content":"","keywords":""},{"title":"Partsâ€‹","type":1,"pageTitle":"Part 2 - Smart cooling","url":"/docs/raspberry-pi/raspberry-pi-cluster/part-2#parts","content":"Noctua NF-A12x25 12v PWM fanSparkFun Logic Level converterABElectronics Breakout PiZero prototyping PCBMolex 47053-1000 4 pin pcb mount socket5.5/2.5mm DC socket12v power supply (from stock)24awg single core wire "},{"title":"Pins usedâ€‹","type":1,"pageTitle":"Part 2 - Smart cooling","url":"/docs/raspberry-pi/raspberry-pi-cluster/part-2#pins-used","content":"GPIO/BCM 18 - for PWM control to fan, via logic level convert to boost 3.3v GPIO to 5vGPIO/BCM 6 - for RPM speed monitor, via logic level converter. Input signal in Hz, two pulses per revolution.  "},{"title":"Designâ€‹","type":1,"pageTitle":"Part 2 - Smart cooling","url":"/docs/raspberry-pi/raspberry-pi-cluster/part-2#design","content":"The SparkFun logic level converter board overview shows that each side of the MOSFET has a 10K ohm pull-up resistor. I'm hoping the RPM speed signal can be read using this from a 5v supply using the high voltage side of the logic level converter. "},{"title":"Electrical Circuit Designâ€‹","type":1,"pageTitle":"Part 2 - Smart cooling","url":"/docs/raspberry-pi/raspberry-pi-cluster/part-2#electrical-circuit-design","content":"The Raspberry Pi has GPIO pins which can provide a Pulse Width Modulation (PWM) signal. Pinout.xyz is a useful resource for working out the pinout and purposes of the GPIO pins which you can use, as well as seeing which pins various HATs, pHATs and SHIMs use when stacking boards together.  Pinout.xyz was useful in confirming which pins the Blinkt! LED array would use which I'll be stacking on top of this PWM fan control board. As we can see from https://pinout.xyz/pinout/blinkt only pins BCM 23 and BCM 24 need to be reserved for the Blinkt! so BCM 18 (PWM0) can be used to generate the PWM signal for the fan and BCM 6 for monitoring the fan speed (BCM 6 was a coin-toss selection, based on it's physical proximity to where the signal wire (at one point) was coming onto the board. The GPIO pins use 3.3v for I/O, and the Noctua fan needs a 5v PWM signal. Fortunately SparkFun make a cheap (Â£3) logic level converter which can translate a signal from a low voltage to a high voltage and vice versa. We can use the Raspberry Pi 5v GPIO to power the high voltage side of the board, as the PWM signal draws very little current.  As the LLC board has pull-up resistors on each side then a second channel can be used for reading the signal from the Noctua fan speed wire, which the Notcua fan PWM specifications show as needing a pull-up resistor. Curcuit design to be added... "},{"title":"Physical Circuit Layoutâ€‹","type":1,"pageTitle":"Part 2 - Smart cooling","url":"/docs/raspberry-pi/raspberry-pi-cluster/part-2#physical-circuit-layout","content":"The Breakout PiZero board was too small for this circuit to wire it nicely. With the logic level converter board being 5 pins wide there was little option but to cross signal and power wires over each other. The wiring under the board is not accessible which I could have used pins and sockets to connect the logic level converter board in a removable way. It does work so it is good enough for purpose, but I'm not proud of the wiring layout which is really not optimal. I chose to use it because it doesn't totally cover the Raspberry Pi 4B it is connected to, which will have CPU load which I don't want throttled due to high CPU temperature. "},{"title":"Testingâ€‹","type":1,"pageTitle":"Part 2 - Smart cooling","url":"/docs/raspberry-pi/raspberry-pi-cluster/part-2#testing","content":"The system daemon code to control the fan will use the python GPIO libraries (assuming they prove suitable). For initial testing from the command line I'm using (the awesome) WiringPi library. Reading Gordon's deprecation notice for WiringPi it makes me sad that open source contributors receive such poor treatment by a number of consumers for the contributions they have made. I'm sure it is a minority of people, but equally sure that fact does not detract from the impact of their behaviour. For reference, here are a few quick commands to get testing with. # Get help for the gpio command man gpio # Check the version of gpio, needs v2.52 for Raspberry Pi 4B support gpio -v # See the pinout for your raspberry pi gpio readall # Set the mode for pin 1 to pwm, rather than tristate logic levels gpio mode 1 pwm # Use gpio pwm &lt;pin&gt; &lt;level&gt; # Level 0 is off, eg 0% duty cycle, level 1023 is 'on', eg 100% duty cycle gpio pwm 1 512  "},{"title":"Troubleshootingâ€‹","type":1,"pageTitle":"Part 2 - Smart cooling","url":"/docs/raspberry-pi/raspberry-pi-cluster/part-2#troubleshooting","content":"My initial circuit would control the fan speed down to it's lowest threshold, but wouldn't turn the fan off completely as it should with a zero duty-cycle (eg. ground) PWM signal to the fan. After sleeping on the problem I realised that I'd not connected the ground from the Pi/Logic level converter to the ground from teh 12v supply which was powering the fan, leaving the PWM &quot;ground&quot; signal floating above the maximum voltage of 0.8v for the PWM low logic level. A bit of rewiring later and things are working as expected. This ground issue was probably also the cause of erroneous RPM readings, which I noticed at the same time as troubleshooting the fan speed. "},{"title":"Future enhancementsâ€‹","type":1,"pageTitle":"Part 2 - Smart cooling","url":"/docs/raspberry-pi/raspberry-pi-cluster/part-2#future-enhancements","content":"Pins and a detachable connector for the 12v supply leads would be useful, as would some insulation between the underside of the board and the conductive aluminium heatsinks on the Raspberry CPU, which could cause a short if not careful to stabalise the board when connecting or removing the 4 pin fan connector. Hopefully adding standoffs to fix the board in place will help, especially as I will enlarge the mounting holes to M3 size. "},{"title":"Resourcesâ€‹","type":1,"pageTitle":"Part 2 - Smart cooling","url":"/docs/raspberry-pi/raspberry-pi-cluster/part-2#resources","content":"SparkFun logic level converterNotcua fan PWM specificationshttps://blog.driftking.tw/en/2019/11/Using-Raspberry-Pi-to-Control-a-PWM-Fan-and-Monitor-its-Speed/ "},{"title":"TypeScript","type":0,"sectionRef":"#","url":"/docs/typescript/overview","content":"","keywords":""},{"title":"Basicsâ€‹","type":1,"pageTitle":"TypeScript","url":"/docs/typescript/overview#basics","content":"tip TypeScript transpiles to JavaScript so at runtime all of the extra type checking etc has been removed. Stop looking for the TypeScript equivalent of casting! :D Use something like ZOD or Valibot if you need/want runtime type validation. "},{"title":"Managing packages and patchingâ€‹","type":1,"pageTitle":"TypeScript","url":"/docs/typescript/overview#managing-packages-and-patching","content":"depcheck or npm-check could be useful (I've not tried either yet) for removing packages which are not used. This isn't the same as npm prune which only removes modules from the node_modules/ folder but not from package.json. "},{"title":"Terraform resources","type":0,"sectionRef":"#","url":"/docs/terraform/resources","content":"Terraform resources InfoQ - Automated infrastructure testingTerragrunt - Keeing IaC DRY","keywords":""},{"title":"TypeScript Types","type":0,"sectionRef":"#","url":"/docs/typescript/types","content":"","keywords":""},{"title":"Utility Typesâ€‹","type":1,"pageTitle":"TypeScript Types","url":"/docs/typescript/types#utility-types","content":"Types can be created from other Types, without having to redefine an entire Type. "},{"title":"Omitâ€‹","type":1,"pageTitle":"TypeScript Types","url":"/docs/typescript/types#omit","content":"Omit - create a Type from another Type with fewer properties. interface Todo { title: string; description: string; completed: boolean; createdAt: number; } type TodoPreview = Omit&lt;Todo, &quot;description&quot;&gt;; const todo: TodoPreview = { title: &quot;Clean room&quot;, completed: false, createdAt: 1615544252770, };  "}]